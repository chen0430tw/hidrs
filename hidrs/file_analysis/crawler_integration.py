#!/usr/bin/env python3
"""
HIDRSçˆ¬è™«æ–‡ä»¶åˆ†æé›†æˆæ¨¡å—

å°†æ–‡ä»¶åˆ†æå™¨é›†æˆåˆ°HIDRSçš„çˆ¬è™«ç³»ç»Ÿä¸­ï¼Œå®ç°:
1. è‡ªåŠ¨åˆ†æçˆ¬å–çš„æ–‡ä»¶
2. å°†åˆ†æç»“æœå­˜å‚¨åˆ°MongoDB
3. é«˜é£é™©æ–‡ä»¶è­¦æŠ¥
4. æ–‡ä»¶å®‰å…¨è¿‡æ»¤

ä½œè€…: HIDRS Team
æ—¥æœŸ: 2026-02-04
"""

import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from pymongo import MongoClient
from .file_analyzer import FileAnalyzer

logger = logging.getLogger(__name__)


class CrawlerFileAnalyzer:
    """
    çˆ¬è™«æ–‡ä»¶åˆ†æå™¨ - é›†æˆHIDRSçˆ¬è™«ç³»ç»Ÿ
    """

    def __init__(self, mongodb_uri: str = 'mongodb://localhost:27017/',
                 db_name: str = 'hidrs_db',
                 auto_delete_high_risk: bool = False):
        """
        åˆå§‹åŒ–çˆ¬è™«æ–‡ä»¶åˆ†æå™¨

        Args:
            mongodb_uri: MongoDBè¿æ¥URI
            db_name: æ•°æ®åº“åç§°
            auto_delete_high_risk: æ˜¯å¦è‡ªåŠ¨åˆ é™¤é«˜é£é™©æ–‡ä»¶
        """
        self.client = MongoClient(mongodb_uri)
        self.db = self.client[db_name]
        self.file_analysis_collection = self.db['file_analysis']
        self.high_risk_alerts = self.db['high_risk_alerts']
        self.auto_delete_high_risk = auto_delete_high_risk

        # åˆ›å»ºç´¢å¼•
        self._create_indexes()

        logger.info(f"æ–‡ä»¶åˆ†æå™¨å·²åˆå§‹åŒ– (DB: {db_name}, è‡ªåŠ¨åˆ é™¤é«˜é£é™©: {auto_delete_high_risk})")

    def _create_indexes(self):
        """
        åˆ›å»ºMongoDBç´¢å¼•
        """
        # æ–‡ä»¶åˆ†æç´¢å¼•
        self.file_analysis_collection.create_index([('file_path', 1)], unique=True)
        self.file_analysis_collection.create_index([('risk_level', 1), ('timestamp', -1)])
        self.file_analysis_collection.create_index([('hashes.sha256', 1)])

        # é«˜é£é™©è­¦æŠ¥ç´¢å¼•
        self.high_risk_alerts.create_index([('timestamp', -1)])
        self.high_risk_alerts.create_index([('file_path', 1)])

        logger.info("MongoDBç´¢å¼•å·²åˆ›å»º")

    def analyze_and_store(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        åˆ†ææ–‡ä»¶å¹¶å­˜å‚¨ç»“æœåˆ°MongoDB

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            metadata: é¢å¤–å…ƒæ•°æ® (çˆ¬è™«æ¥æºã€URLç­‰)

        Returns:
            åˆ†æç»“æœ
        """
        try:
            # åˆ†ææ–‡ä»¶
            logger.info(f"æ­£åœ¨åˆ†ææ–‡ä»¶: {file_path}")
            analyzer = FileAnalyzer(file_path)
            result = analyzer.analyze()

            # æ·»åŠ å…ƒæ•°æ®
            result['file_path'] = file_path
            result['crawler_metadata'] = metadata or {}

            # å­˜å‚¨åˆ°MongoDB
            self.file_analysis_collection.update_one(
                {'file_path': file_path},
                {'$set': result},
                upsert=True
            )

            logger.info(f"æ–‡ä»¶åˆ†æå®Œæˆ: {file_path} (é£é™©ç­‰çº§: {result['risk_assessment']['risk_level']})")

            # é«˜é£é™©å¤„ç†
            if result['risk_assessment']['risk_level'] in ['high', 'medium']:
                self._handle_high_risk_file(file_path, result)

            return result

        except Exception as e:
            logger.error(f"æ–‡ä»¶åˆ†æå¤±è´¥: {file_path} - {str(e)}")
            return {'error': str(e), 'file_path': file_path}

    def _handle_high_risk_file(self, file_path: str, analysis_result: Dict[str, Any]):
        """
        å¤„ç†é«˜é£é™©æ–‡ä»¶
        """
        risk = analysis_result['risk_assessment']

        # è®°å½•è­¦æŠ¥
        alert = {
            'file_path': file_path,
            'file_name': os.path.basename(file_path),
            'risk_level': risk['risk_level'],
            'risk_score': risk['risk_score'],
            'risk_factors': risk['risk_factors'],
            'recommendation': risk['recommendation'],
            'file_hash_sha256': analysis_result['hashes']['sha256'],
            'timestamp': datetime.now(),
            'handled': False,
            'action_taken': None
        }

        self.high_risk_alerts.insert_one(alert)
        logger.warning(f"âš ï¸ é«˜é£é™©æ–‡ä»¶æ£€æµ‹: {file_path} ({risk['risk_level']})")

        # è‡ªåŠ¨åˆ é™¤
        if self.auto_delete_high_risk and risk['risk_level'] == 'high':
            try:
                os.remove(file_path)
                logger.warning(f"ğŸ—‘ï¸ é«˜é£é™©æ–‡ä»¶å·²åˆ é™¤: {file_path}")
                self.high_risk_alerts.update_one(
                    {'file_path': file_path},
                    {'$set': {'action_taken': 'deleted', 'handled': True}}
                )
            except Exception as e:
                logger.error(f"åˆ é™¤é«˜é£é™©æ–‡ä»¶å¤±è´¥: {file_path} - {str(e)}")

    def batch_analyze(self, file_paths: List[str], metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡åˆ†ææ–‡ä»¶

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata: å…¬å…±å…ƒæ•°æ®

        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []

        for file_path in file_paths:
            try:
                result = self.analyze_and_store(file_path, metadata)
                results.append(result)
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {file_path} - {str(e)}")
                results.append({'error': str(e), 'file_path': file_path})

        logger.info(f"æ‰¹é‡åˆ†æå®Œæˆ: {len(results)}/{len(file_paths)} ä¸ªæ–‡ä»¶")
        return results

    def get_high_risk_files(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        è·å–é«˜é£é™©æ–‡ä»¶åˆ—è¡¨

        Args:
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            é«˜é£é™©æ–‡ä»¶åˆ—è¡¨
        """
        return list(self.high_risk_alerts.find().sort('timestamp', -1).limit(limit))

    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶åˆ†æç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        pipeline = [
            {
                '$group': {
                    '_id': '$risk_assessment.risk_level',
                    'count': {'$sum': 1}
                }
            }
        ]

        risk_distribution = list(self.file_analysis_collection.aggregate(pipeline))

        return {
            'total_files_analyzed': self.file_analysis_collection.count_documents({}),
            'high_risk_alerts': self.high_risk_alerts.count_documents({}),
            'unhandled_alerts': self.high_risk_alerts.count_documents({'handled': False}),
            'risk_distribution': {item['_id']: item['count'] for item in risk_distribution},
            'last_updated': datetime.now().isoformat()
        }

    def query_by_hash(self, file_hash: str, hash_type: str = 'sha256') -> Optional[Dict[str, Any]]:
        """
        é€šè¿‡å“ˆå¸Œå€¼æŸ¥è¯¢æ–‡ä»¶åˆ†æç»“æœ

        Args:
            file_hash: æ–‡ä»¶å“ˆå¸Œå€¼
            hash_type: å“ˆå¸Œç±»å‹ (md5, sha1, sha256, sha512)

        Returns:
            åˆ†æç»“æœæˆ–None
        """
        query_key = f'hashes.{hash_type}'
        return self.file_analysis_collection.find_one({query_key: file_hash})

    def close(self):
        """
        å…³é—­æ•°æ®åº“è¿æ¥
        """
        self.client.close()
        logger.info("æ•°æ®åº“è¿æ¥å·²å…³é—­")


def integrate_with_crawler(crawler_instance, mongodb_uri: str = 'mongodb://localhost:27017/',
                          auto_delete_high_risk: bool = False):
    """
    å°†æ–‡ä»¶åˆ†æå™¨é›†æˆåˆ°çˆ¬è™«å®ä¾‹

    Args:
        crawler_instance: çˆ¬è™«å®ä¾‹
        mongodb_uri: MongoDBè¿æ¥URI
        auto_delete_high_risk: æ˜¯å¦è‡ªåŠ¨åˆ é™¤é«˜é£é™©æ–‡ä»¶

    Returns:
        CrawlerFileAnalyzerå®ä¾‹
    """
    file_analyzer = CrawlerFileAnalyzer(mongodb_uri, auto_delete_high_risk=auto_delete_high_risk)

    # è·å–çˆ¬è™«çš„ä¸‹è½½ç›®å½•
    download_dir = getattr(crawler_instance, 'download_dir', './downloads')

    logger.info(f"æ–‡ä»¶åˆ†æå™¨å·²é›†æˆåˆ°çˆ¬è™« (ç›‘æ§ç›®å½•: {download_dir})")

    return file_analyzer


if __name__ == '__main__':
    # æµ‹è¯•é›†æˆ
    logging.basicConfig(level=logging.INFO)

    analyzer = CrawlerFileAnalyzer()

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = analyzer.get_statistics()
    print("æ–‡ä»¶åˆ†æç»Ÿè®¡:")
    print(f"  æ€»åˆ†ææ–‡ä»¶æ•°: {stats['total_files_analyzed']}")
    print(f"  é«˜é£é™©è­¦æŠ¥æ•°: {stats['high_risk_alerts']}")
    print(f"  æœªå¤„ç†è­¦æŠ¥æ•°: {stats['unhandled_alerts']}")
    print(f"  é£é™©åˆ†å¸ƒ: {stats['risk_distribution']}")

    # è·å–é«˜é£é™©æ–‡ä»¶
    high_risk_files = analyzer.get_high_risk_files(limit=10)
    if high_risk_files:
        print(f"\næœ€è¿‘{len(high_risk_files)}ä¸ªé«˜é£é™©æ–‡ä»¶:")
        for alert in high_risk_files:
            print(f"  â€¢ {alert['file_name']} ({alert['risk_level']}) - {alert['timestamp']}")

    analyzer.close()
