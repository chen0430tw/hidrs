# HIDRS Common Crawl æ•°æ®æ¥å…¥ç³»ç»Ÿ

## ğŸ¯ åŠŸèƒ½æ¦‚è¿°

å°†HIDRSä¸Common Crawlé›†æˆï¼Œå®ç°ç±»XKeyscoreçš„å¤§è§„æ¨¡å†å²ç½‘é¡µæœç´¢å’Œåˆ†æåŠŸèƒ½ã€‚

**æ ¸å¿ƒèƒ½åŠ›**ï¼š
- ğŸ“¦ æµå¼å¤„ç†PBçº§WARCæ•°æ®ï¼ˆæ— éœ€å®Œæ•´ä¸‹è½½ï¼‰
- ğŸ” æœç´¢30-50äº¿ç½‘é¡µçš„å†å²å¿«ç…§
- ğŸ§  é›†æˆHLIGæ‹‰æ™®æ‹‰æ–¯è°±åˆ†æ
- ğŸ’¾ MongoDBæµå¼å¯¼å…¥ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰
- ğŸ¨ ç±»XKeyscoreé«˜çº§æŸ¥è¯¢ç•Œé¢

## ğŸ†š ä¸XKeyscoreå¯¹æ¯”

| åŠŸèƒ½ | XKeyscore | HIDRS + Common Crawl |
|------|-----------|---------------------|
| æ•°æ®æ¥æº | å®æ—¶æµé‡æ‹¦æˆªï¼ˆéæ³•ï¼‰ | å…¬å¼€ç½‘é¡µå¿«ç…§ï¼ˆåˆæ³•ï¼‰ |
| æ•°æ®è§„æ¨¡ | 20 TB/å¤© | 100 TB/æœˆ |
| å†å²æ•°æ® | 3-5å¤© | **æ°¸ä¹…å­˜å‚¨** |
| æœç´¢åŠŸèƒ½ | âœ… | âœ… |
| èšç±»åˆ†æ | åŸºç¡€ | **HLIGå¢å¼º** |
| è¯­ä¹‰æ£€ç´¢ | âŒ | **âœ…** |
| åˆæ³•æ€§ | âš ï¸ æœ‰äº‰è®® | **âœ… å®Œå…¨åˆæ³•** |

## ğŸ“š æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç”¨æˆ·æŸ¥è¯¢ç•Œé¢ (Web UI)                      â”‚
â”‚         (ç±»ä¼¼XKeyscoreçš„é«˜çº§æœç´¢ç•Œé¢)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          HIDRS æŸ¥è¯¢å¼•æ“ + åˆ†æå¸ˆå·¥ä½œå°                  â”‚
â”‚  â€¢ æ‹‰æ™®æ‹‰æ–¯è°±åˆ†æ                                       â”‚
â”‚  â€¢ å…¨æ¯æ˜ å°„æ£€ç´¢                                         â”‚
â”‚  â€¢ Query Builder (å¤šæ¡ä»¶æŸ¥è¯¢)                          â”‚
â”‚  â€¢ èšç±»åˆ†æ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MongoDB / Elasticsearch é›†ç¾¤                 â”‚
â”‚  â€¢ ç´¢å¼•å±‚ï¼šURLã€åŸŸåã€æ—¶é—´ã€å…³é”®è¯                       â”‚
â”‚  â€¢ å…¨æ–‡æœç´¢ï¼šæ ‡é¢˜ã€å†…å®¹ã€å…ƒæ•°æ®                          â”‚
â”‚  â€¢ å‘é‡åµŒå…¥ï¼šè¯­ä¹‰æ£€ç´¢                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Common Crawl æ•°æ®ä»“åº“ (50+ PB)                   â”‚
â”‚  â€¢ WARCæ–‡ä»¶å­˜å‚¨ (Amazon S3)                           â”‚
â”‚  â€¢ æ¯æœˆå¿«ç…§ï¼š2024-01, 2024-02, ...                    â”‚
â”‚  â€¢ å…¨çƒ30-50äº¿ç½‘é¡µ                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£…Common Crawlç›¸å…³åº“
pip install warcio comcrawl boto3

# å®‰è£…MongoDB
docker run -d -p 27017:27017 --name mongodb mongo

# å®‰è£…å…¶ä»–ä¾èµ–
pip install -r hidrs/requirements.txt
```

### 2. åŸºæœ¬ä½¿ç”¨

#### æœç´¢ç´¢å¼•

```python
from hidrs.commoncrawl import CommonCrawlIndexClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = CommonCrawlIndexClient()

# æœç´¢Wikipediaç›¸å…³é¡µé¢
results = client.search(
    url_pattern='*.wikipedia.org/*',
    limit=100,
    filter_status=[200],
)

print(f"æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
for result in results:
    print(f"{result['url']} - {result['timestamp']}")
```

#### æµå¼è§£æWARC

```python
from hidrs.commoncrawl import WARCStreamParser

# åˆ›å»ºè§£æå™¨
parser = WARCStreamParser()

# æµå¼å¤„ç†WARCæ–‡ä»¶ï¼ˆæ— éœ€å®Œæ•´ä¸‹è½½ï¼‰
warc_url = "https://data.commoncrawl.org/crawl-data/..."

for record in parser.stream_warc(warc_url):
    print(f"URL: {record['url']}")
    print(f"æ ‡é¢˜: {record['title']}")
    print(f"æ–‡æœ¬: {record['text'][:100]}...")
```

#### å¯¼å…¥æ•°æ®åˆ°MongoDB

```python
from hidrs.commoncrawl import CommonCrawlImporter

# åˆ›å»ºå¯¼å…¥å™¨
importer = CommonCrawlImporter(
    mongo_uri='mongodb://localhost:27017/',
    database='hidrs_commoncrawl',
    batch_size=1000,
    enable_hlig_analysis=True,  # å¯ç”¨HLIGåˆ†æ
)

# å¯¼å…¥æ•°æ®
stats = importer.import_from_url_pattern(
    url_pattern='*.example.com/*',
    limit=10000,
)

print(f"å¯¼å…¥å®Œæˆ: {stats['inserted']} æ¡è®°å½•")
```

#### é«˜çº§æŸ¥è¯¢

```python
from hidrs.commoncrawl import CommonCrawlQueryEngine

# åˆ›å»ºæŸ¥è¯¢å¼•æ“
engine = CommonCrawlQueryEngine(
    mongo_uri='mongodb://localhost:27017/',
    database='hidrs_commoncrawl',
)

# é«˜çº§å¤šæ¡ä»¶æŸ¥è¯¢ï¼ˆç±»XKeyscoreï¼‰
results = engine.advanced_search(
    keywords=["ç½‘ç»œæ”»å‡»", "APT"],
    domain="*.gov.cn",
    from_date="2024-01-01",
    to_date="2024-12-31",
    status_codes=[200],
    limit=1000
)

print(f"æ‰¾åˆ° {len(results)} æ¡ç»“æœ")

# èšç±»åˆ†æ
clusters = engine.cluster_results(results, n_clusters=5)
print(f"è¯†åˆ«å‡º {len(clusters['clusters'])} ä¸ªç°‡")

# æ—¶é—´çº¿åˆ†æ
timeline = engine.get_timeline({}, interval='day')
for point in timeline[:10]:
    print(f"{point['timestamp']}: {point['count']} æ¡è®°å½•")
```

## ğŸ“Š å®Œæ•´æ¼”ç¤º

è¿è¡Œå®Œæ•´æ¼”ç¤ºç¨‹åºï¼š

```bash
python examples/commoncrawl_demo.py
```

æ¼”ç¤ºåŒ…æ‹¬ï¼š
1. âœ… æœç´¢Common Crawlç´¢å¼•
2. âœ… æµå¼è§£æWARCæ–‡ä»¶
3. âœ… å¯¼å…¥æ•°æ®åˆ°MongoDB
4. âœ… é«˜çº§å¤šæ¡ä»¶æŸ¥è¯¢
5. âœ… èšç±»åˆ†æï¼ˆHLIGï¼‰
6. âœ… å®Œæ•´å·¥ä½œæµï¼ˆå¨èƒæƒ…æŠ¥æ”¶é›†ï¼‰

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. WARCStreamParser - æµå¼WARCè§£æå™¨

**åŠŸèƒ½**ï¼š
- æµå¼è¯»å–WARCæ–‡ä»¶ï¼ˆæ— éœ€å®Œæ•´ä¸‹è½½ï¼‰
- è‡ªåŠ¨è§£å‹gzipæ ¼å¼
- HTMLè§£æï¼ˆBeautifulSoupï¼‰
- æ–‡æœ¬æå–ã€é“¾æ¥æå–
- å¤šçº¿ç¨‹æ‰¹é‡å¤„ç†

**ä¼˜åŒ–**ï¼š
- å¢é‡è§£æï¼ˆæ¯æ¬¡åªåŠ è½½ä¸€ä¸ªrecordï¼‰
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- å†…å­˜å ç”¨æ§åˆ¶

### 2. CommonCrawlIndexClient - ç´¢å¼•æœç´¢å®¢æˆ·ç«¯

**åŠŸèƒ½**ï¼š
- æœç´¢Common Crawl CDXç´¢å¼•
- å¤šå¤‡ç”¨æ¥å£ï¼ˆcomcrawl / CDX API / boto3ï¼‰
- è‡ªåŠ¨é™çº§ç­–ç•¥
- æ—¶é—´èŒƒå›´ç­›é€‰
- çŠ¶æ€ç è¿‡æ»¤

**æ¥å£ä¼˜å…ˆçº§**ï¼š
1. comcrawlï¼ˆæ¨èï¼Œæœ€ç®€å•ï¼‰
2. CDX APIï¼ˆå¤‡ç”¨ï¼Œæ— ä¾èµ–ï¼‰
3. boto3 S3ç›´è¿ï¼ˆå¤§è§„æ¨¡å¤„ç†ï¼‰

### 3. CommonCrawlImporter - æ•°æ®å¯¼å…¥å™¨

**åŠŸèƒ½**ï¼š
- MongoDBæµå¼å¯¼å…¥
- æ‰¹é‡å†™å…¥ä¼˜åŒ–ï¼ˆbatch_size=1000ï¼‰
- å¼‚æ­¥å†™å…¥é˜Ÿåˆ—
- é‡å¤æ•°æ®å»é‡
- HLIGæ‹‰æ™®æ‹‰æ–¯åˆ†æé›†æˆ
- è‡ªåŠ¨å…³é”®è¯æå–

**æ€§èƒ½**ï¼š
- æ‰¹é‡upsertæ“ä½œ
- å¤šçº¿ç¨‹å¹¶å‘
- è¿›åº¦è¿½è¸ª

### 4. CommonCrawlQueryEngine - æŸ¥è¯¢å¼•æ“

**åŠŸèƒ½**ï¼š
- å¤šæ¡ä»¶å¤åˆæŸ¥è¯¢
- å…¨æ–‡æœç´¢ï¼ˆMongoDB text indexï¼‰
- æ—¶é—´èŒƒå›´ç­›é€‰
- åŸŸå/TLDç­›é€‰
- èšç±»åˆ†æï¼ˆHLIGï¼‰
- æ—¶é—´çº¿è¶‹åŠ¿åˆ†æ
- åŸŸåç»Ÿè®¡

**æŸ¥è¯¢èƒ½åŠ›**ï¼š
- URLç²¾ç¡®åŒ¹é…/é€šé…ç¬¦
- å…³é”®è¯AND/ORé€»è¾‘
- HTTPçŠ¶æ€ç è¿‡æ»¤
- Content-Typeè¿‡æ»¤
- åœ°ç†ä½ç½®ï¼ˆTLDï¼‰

## ğŸ’¡ å®é™…åº”ç”¨åœºæ™¯

### 1. å¼€æºæƒ…æŠ¥æ”¶é›† (OSINT)

```python
# è¿½è¸ªAPTç»„ç»‡C2æœåŠ¡å™¨å†å²
results = engine.advanced_search(
    domain="*.suspicious-domain.com",
    from_date="2023-01-01",
    to_date="2024-12-31",
)

# æå–IOC
unique_ips = set(r.get('server_ip') for r in results if r.get('server_ip'))
print(f"å‘ç° {len(unique_ips)} ä¸ªå”¯ä¸€IP")
```

### 2. å“ç‰Œç›‘æ§

```python
# å‘ç°å±±å¯¨ç½‘ç«™
results = engine.search("å‡å†’å“ç‰Œ", limit=1000)
clusters = engine.cluster_results(results)

# æŒ‰åŸŸååˆ†ç»„
for cluster in clusters['clusters']:
    print(f"åŸŸå: {cluster['domain']}, æ•°é‡: {cluster['size']}")
```

### 3. å­¦æœ¯ç ”ç©¶

```python
# äº’è”ç½‘è€ƒå¤å­¦ï¼šç ”ç©¶ç½‘é¡µå†…å®¹å˜è¿
timeline = engine.get_timeline(
    query={'domain': 'archive.org'},
    interval='month'
)

# åˆ†æè¶‹åŠ¿
for point in timeline:
    print(f"{point['timestamp']}: {point['count']} æ¬¡å¿«ç…§")
```

### 4. å®‰å…¨ç ”ç©¶

```python
# å‘ç°æš´éœ²çš„æ•æ„Ÿä¿¡æ¯
results = engine.advanced_search(
    keywords=["password", "api_key", "secret"],
    content_types=["text/plain", "application/json"],
    limit=5000
)

# åˆ†ææ³„éœ²æ¨¡å¼
for result in results:
    if "password" in result['text']:
        print(f"âš ï¸ æ½œåœ¨å¯†ç æ³„éœ²: {result['url']}")
```

## ğŸ¨ ç±»XKeyscoreæŸ¥è¯¢ç¤ºä¾‹

### åŸºç¡€æŸ¥è¯¢

```python
# æŸ¥æ‰¾.gov.cnåŸŸåä¸­åŒ…å«"ç½‘ç»œæ”»å‡»"çš„é¡µé¢
results = engine.advanced_search(
    keywords=["ç½‘ç»œæ”»å‡»"],
    tld=".gov.cn",
    status_codes=[200],
)
```

### å¤æ‚æŸ¥è¯¢

```python
# æŸ¥æ‰¾2024å¹´1-3æœˆé—´ï¼ŒGitHubä¸Šæ‰€æœ‰åŒ…å«"æ¼æ´"å­—æ ·çš„issueé¡µé¢
results = engine.advanced_search(
    keywords=["æ¼æ´", "vulnerability"],
    url="https://github.com/*/issues/*",
    from_date="2024-01-01",
    to_date="2024-03-31",
    status_codes=[200],
    limit=5000
)
```

### æ—¶é—´çº¿åˆ†æ

```python
# åˆ†ææŸä¸ªè¯é¢˜çš„çƒ­åº¦è¶‹åŠ¿
timeline = engine.get_timeline(
    query={'keywords': {'$regex': 'äººå·¥æ™ºèƒ½'}},
    interval='month'
)

# ç»˜åˆ¶è¶‹åŠ¿å›¾
import matplotlib.pyplot as plt
dates = [p['timestamp'] for p in timeline]
counts = [p['count'] for p in timeline]
plt.plot(dates, counts)
plt.title('äººå·¥æ™ºèƒ½è¯é¢˜çƒ­åº¦è¶‹åŠ¿')
plt.show()
```

## ğŸ’° æˆæœ¬ä¼°ç®—

### å°è§„æ¨¡æµ‹è¯•ï¼ˆ10ä¸‡ç½‘é¡µï¼‰

```
æ•°æ®é‡: ~10 GB
MongoDBå­˜å‚¨: $0.25/GB/æœˆ = $2.5/æœˆ
è®¡ç®—èµ„æº: å¯å¿½ç•¥
æ€»æˆæœ¬: ~$5/æœˆ
```

### ä¸­ç­‰è§„æ¨¡ï¼ˆ100ä¸‡ç½‘é¡µï¼‰

```
æ•°æ®é‡: ~100 GB
MongoDBå­˜å‚¨: $25/æœˆ
Elasticsearch: $50/æœˆ
è®¡ç®—èµ„æº: $20/æœˆ
æ€»æˆæœ¬: ~$100/æœˆ
```

### å¤§è§„æ¨¡ï¼ˆ1000ä¸‡ç½‘é¡µï¼‰

```
æ•°æ®é‡: ~1 TB
MongoDB Atlas (M200): $2,000/æœˆ
Elasticsearch (m5.4xlarge): $800/æœˆ
è®¡ç®—èµ„æº: $200/æœˆ
æ€»æˆæœ¬: ~$3,000/æœˆ
```

### è¶…å¤§è§„æ¨¡ï¼ˆ1äº¿ç½‘é¡µ+ï¼‰

```
æ•°æ®é‡: ~10 TB
MongoDB Atlas (M400 Cluster): $6,500/æœˆ
Elasticsearch (m5.12xlarge Ã— 3): $5,000/æœˆ
è®¡ç®—èµ„æº: $1,000/æœˆ
æ€»æˆæœ¬: ~$12,500/æœˆ
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### æ•°æ®éšç§

âœ… **å®Œå…¨åˆæ³•**ï¼š
- Common Crawlæ•°æ®æ˜¯å…¬å¼€çš„
- æ‰€æœ‰ç½‘é¡µå‡ä¸ºå…¬å¼€å¯è®¿é—®
- éµå®ˆrobots.txtè§„èŒƒ

âš ï¸ **ä½¿ç”¨é™åˆ¶**ï¼š
- ä¸å¾—ç”¨äºä¾µçŠ¯éšç§
- ä¸å¾—ç”¨äºéæ³•ç›®çš„
- éµå®ˆæœ¬åœ°æ³•å¾‹æ³•è§„

### æŠ€æœ¯é™åˆ¶

- **æ•°æ®å»¶è¿Ÿ**ï¼šæ¯æœˆæ›´æ–°1æ¬¡ï¼Œæœ€æ–°æ•°æ®æœ‰1ä¸ªæœˆå»¶è¿Ÿ
- **è¦†ç›–ä¸å®Œæ•´**ï¼šæŸäº›ç½‘ç«™ç¦æ­¢çˆ¬å–
- **éœ€è¦ç™»å½•çš„å†…å®¹**ï¼šæ— æ³•è·å–
- **åŠ¨æ€åŠ è½½**ï¼šJavaScriptæ¸²æŸ“çš„å†…å®¹å¯èƒ½ä¸¢å¤±

### æ€§èƒ½ä¼˜åŒ–

1. **MongoDBç´¢å¼•**ï¼šç¡®ä¿åˆ›å»ºåˆé€‚çš„ç´¢å¼•
2. **æ‰¹é‡æ“ä½œ**ï¼šä½¿ç”¨bulk_writeè€Œéå•æ¡æ’å…¥
3. **æµå¼å¤„ç†**ï¼šé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§æ–‡ä»¶
4. **åˆ†ç‰‡é›†ç¾¤**ï¼šå¤§è§„æ¨¡æ•°æ®ä½¿ç”¨MongoDBåˆ†ç‰‡

## ğŸ“– å‚è€ƒèµ„æ–™

### Common Crawl

- å®˜ç½‘: https://commoncrawl.org/
- æ•°æ®ä¸‹è½½: https://data.commoncrawl.org/
- WARCæ ¼å¼: https://iipc.github.io/warc-specifications/

### Pythonåº“

- warcio: https://github.com/webrecorder/warcio
- comcrawl: https://pypi.org/project/comcrawl/
- cc-pyspark: https://github.com/commoncrawl/cc-pyspark

### HIDRSç›¸å…³

- HLIGç†è®º: `/home/user/hidrs/CLAUDE.md`
- XKeyscoreå¯¹æ¯”: `/home/user/hidrs/XKEYSCORE-VS-XKEYSTROKE.md`
- å¯è¡Œæ€§åˆ†æ: `/home/user/hidrs/HIDRS-COMMONCRAWL-XKEYSCORE-ANALYSIS.md`

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

---

**ç‰ˆæœ¬**: 1.0.0
**ä½œè€…**: HIDRS Team
**åˆ›å»ºæ—¥æœŸ**: 2026-02-06
