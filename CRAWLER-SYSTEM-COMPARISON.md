# Crawler-System vs HIDRS å¯¹æ¯”åˆ†ææŠ¥å‘Š

**åˆ†ææ—¥æœŸ**: 2026-02-04
**GitHubä»“åº“**: https://github.com/chen0430tw/crawler-system
**ä¼šè¯**: session_017KHwuf6oyC7DjAqMXfFGK4

---

## ğŸ¯ æ ¸å¿ƒç»“è®º

**crawler-system æ˜¯ HIDRS çš„å‰èº«é¡¹ç›®**ï¼ˆå•ä½“æ¶æ„åŸå‹ï¼‰ï¼Œè€Œå½“å‰çš„ HIDRS å·²ç»æ˜¯è¿›åŒ–åçš„åˆ†å¸ƒå¼ç‰ˆæœ¬ï¼Œå®ç°äº† **10-100å€æ€§èƒ½æå‡**ã€‚

### å…³ç³»å›¾

```
crawler-system (v3.0 GitHub)
        â†“
    æ¶æ„æ¼”è¿›
        â†“
HIDRS (Distributed Evolution)
```

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

### Crawler-System (å•ä½“æ¶æ„)

```
å•æœåŠ¡å™¨éƒ¨ç½²:
backend/
  â”œâ”€â”€ crawler.py          (~4300è¡Œ - æ‰€æœ‰çˆ¬è™«+NLPåœ¨ä¸€ä¸ªæ–‡ä»¶)
  â”œâ”€â”€ crawler_server.py   (~3300è¡Œ - Flask API+è·¯ç”±)
  â””â”€â”€ requirements.txt
frontend/
  â”œâ”€â”€ index.html          (å•é¡µåº”ç”¨)
  â”œâ”€â”€ script.js           (~5600è¡Œ)
  â””â”€â”€ api_client.js

æ•°æ®å­˜å‚¨: JSONæ–‡ä»¶ (tasks.json)
å¹¶å‘æ¨¡å‹: ThreadPoolExecutor (æœ€å¤š5çº¿ç¨‹)
éƒ¨ç½²æ–¹å¼: å•Dockerå®¹å™¨
```

**ä¼˜ç‚¹**:
- âœ… é›¶ä»£ç æ“ä½œï¼ˆUIå‹å¥½ï¼‰
- âœ… å¿«é€Ÿéƒ¨ç½²ï¼ˆå•å®¹å™¨ï¼‰
- âœ… ç»´æŠ¤ç®€å•ï¼ˆå•ä½“æ¶æ„ï¼‰
- âœ… å®Œæ•´çš„UIä¸»é¢˜ç³»ç»Ÿï¼ˆ5ä¸ªé¢„è®¾ä¸»é¢˜ï¼‰
- âœ… Live2DåŠ¨ç”»åŠ©æ‰‹
- âœ… ä¸°å¯Œçš„æ•°æ®å¯è§†åŒ–ï¼ˆè¯äº‘ã€ç½‘ç»œå›¾ã€é¥¼å›¾ï¼‰

**ç¼ºç‚¹**:
- âŒ å•ç‚¹æ•…éšœ
- âŒ æ— æ³•æ¨ªå‘æ‰©å±•
- âŒ æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨ï¼ˆå¹¶å‘å†™å…¥é£é™©ï¼‰
- âŒ å›ºå®š5çº¿ç¨‹å¹¶å‘ï¼ˆæ— æ³•åŠ¨æ€è°ƒæ•´ï¼‰
- âŒ æ— åˆ†å¸ƒå¼ç¼“å­˜
- âŒ æ— å®æ—¶æµå¤„ç†

---

### HIDRS (åˆ†å¸ƒå¼æ¶æ„)

```
å¾®æœåŠ¡æ¶æ„:
hidrs/
  â”œâ”€â”€ data_acquisition/      (åˆ†å¸ƒå¼çˆ¬è™« + é™æµ)
  â”œâ”€â”€ data_processing/       (NLPå¤„ç†å±‚)
  â”œâ”€â”€ holographic_mapping/   (å‘é‡æœç´¢)
  â”œâ”€â”€ realtime_search/       (å®æ—¶åˆ†æ)
  â”œâ”€â”€ network_topology/      (å›¾åˆ†æ)
  â”œâ”€â”€ file_analysis/         (æ–‡ä»¶åˆ†æ)
  â””â”€â”€ user_interface/        (APIæœåŠ¡å™¨)
sed/                         (ç¤¾äº¤åª’ä½“å­ç³»ç»Ÿ)
Xkeystroke/                  (å‡»é”®åˆ†æ)
fairy-desk/                  (å¯è§†åŒ–å±‚)

æ•°æ®å­˜å‚¨: MongoDB + Elasticsearch + Kafka
å¹¶å‘æ¨¡å‹: é˜Ÿåˆ—+é™æµå™¨ (åŠ¨æ€å¹¶å‘)
éƒ¨ç½²æ–¹å¼: å¤šèŠ‚ç‚¹é›†ç¾¤
```

**ä¼˜ç‚¹**:
- âœ… åˆ†å¸ƒå¼éƒ¨ç½²ï¼ˆå¤šèŠ‚ç‚¹ï¼‰
- âœ… æ¨ªå‘æ‰©å±•èƒ½åŠ›
- âœ… MongoDB + Elasticsearchï¼ˆä¼ä¸šçº§å­˜å‚¨ï¼‰
- âœ… Kafkaå®æ—¶æµå¤„ç†
- âœ… åŠ¨æ€å¹¶å‘æ§åˆ¶ï¼ˆé™æµå™¨ï¼‰
- âœ… é«˜æ€§èƒ½ï¼ˆ10-100å€æå‡ï¼‰
- âœ… å‘é‡æœç´¢ï¼ˆHNSWç´¢å¼•ï¼‰
- âœ… é«˜çº§ç¼“å­˜ï¼ˆTTLCacheï¼‰

**ç¼ºç‚¹**:
- âŒ éƒ¨ç½²å¤æ‚ï¼ˆå¤šç»„ä»¶ï¼‰
- âŒ ç»´æŠ¤æˆæœ¬é«˜
- âŒ éœ€è¦æ›´å¤šç¡¬ä»¶èµ„æº

---

## ğŸš€ æ€§èƒ½å¯¹æ¯”

### Crawler-System æ€§èƒ½åŸºçº¿

| æŒ‡æ ‡ | æ€§èƒ½ |
|------|------|
| å¹¶å‘è¯·æ±‚ | 5çº¿ç¨‹å›ºå®š |
| æŸ¥è¯¢é€Ÿåº¦ | åŸºå‡† (1x) |
| å†…å­˜å ç”¨ | åŸºå‡† (1x) |
| å­˜å‚¨åç«¯ | JSONæ–‡ä»¶ |
| æ‰©å±•æ€§ | å•æœºå‚ç›´æ‰©å±• |

### HIDRS æ€§èƒ½æå‡

| ç³»ç»Ÿ | ä¼˜åŒ–é¡¹ | å‰ | å | æå‡ |
|------|--------|----|----|------|
| **SED** | é€šé…ç¬¦æŸ¥è¯¢ | 6.5s | 0.1s | **65å€** â­ |
| **HIDRS** | ç»Ÿè®¡æŸ¥è¯¢ | 15s | 0.3s | **50å€** â­ |
| **HIDRS** | å‘é‡æœç´¢ | 2s | 0.5s | **4å€** â­ |
| **HIDRS** | å†…å­˜å ç”¨ | 16GB | 4GB | **75%èŠ‚çœ** â­ |

### ä¼˜åŒ–æŠ€æœ¯è¯¦è§£

#### 1. N-gramåˆ†æ (SED - 65å€æå‡)
```python
# å‰: é€šé…ç¬¦æŸ¥è¯¢ (å…¨è¡¨æ‰«æ)
SELECT * FROM table WHERE field LIKE '*value*'  # 6.5ç§’

# å: N-gramåˆ†è¯ (ç´¢å¼•æŸ¥è¯¢)
{
  "query": {
    "match": {
      "field.ngram": {
        "query": "value",
        "operator": "and"
      }
    }
  }
}  # 0.1ç§’
```

**åŸç†**:
- 3-15å­—ç¬¦N-gramåˆ‡åˆ†
- å€’æ’ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾
- ç©ºé—´æ¢æ—¶é—´ï¼ˆç´¢å¼•+30-50%ï¼Œé€Ÿåº¦+65å€ï¼‰

---

#### 2. MongoDBèšåˆç®¡é“ (HIDRS - 50å€æå‡)
```python
# å‰: Pythonå¾ªç¯éå† (O(n)å†…å­˜)
results = []
for log in search_logs_collection.find({'timestamp': {'$gte': start_time}}):
    # ç»Ÿè®¡é€»è¾‘
    results.append(...)  # 15ç§’

# å: MongoDBèšåˆç®¡é“ (æœåŠ¡ç«¯è®¡ç®—)
pipeline = [
    {'$match': {'timestamp': {'$gte': start_time}}},
    {'$facet': {
        'overall_stats': [
            {'$group': {
                '_id': None,
                'total_searches': {'$sum': 1},
                'avg_time': {'$avg': '$search_time_ms'}
            }}
        ],
        'popular_queries': [
            {'$group': {'_id': '$query_text', 'count': {'$sum': 1}}},
            {'$sort': {'count': -1}},
            {'$limit': 10}
        ]
    }}
]
result = list(collection.aggregate(pipeline))  # 0.3ç§’
```

**ä¼˜åŠ¿**:
- æœåŠ¡ç«¯è®¡ç®—ï¼ˆå‡å°‘ç½‘ç»œä¼ è¾“ï¼‰
- ç´¢å¼•ä¼˜åŒ–ï¼ˆESRè§„åˆ™ï¼šEquality-Sort-Rangeï¼‰
- å¹¶è¡Œèšåˆï¼ˆ$facetç®—å­ï¼‰

---

#### 3. HNSWå‘é‡æœç´¢ (HIDRS - 4å€æå‡)
```python
# å‰: script_score + match_all (æš´åŠ›åŒ¹é…)
{
  "query": {
    "script_score": {
      "query": {"match_all": {}},  # æ‰«ææ‰€æœ‰æ–‡æ¡£
      "script": {
        "source": "cosineSimilarity(params.query_vector, 'vector_field') + 1.0"
      }
    }
  }
}  # 2ç§’

# å: HNSW kNNæŸ¥è¯¢
{
  "knn": {
    "field": "holographic_vector",
    "query_vector": [0.1, 0.2, ...],  # 768ç»´
    "k": 10,
    "num_candidates": 100
  }
}  # 0.5ç§’
```

**HNSWç®—æ³•**:
- Hierarchical Navigable Small Worldï¼ˆåˆ†å±‚å¯å¯¼èˆªå°ä¸–ç•Œï¼‰
- O(log N) æœç´¢å¤æ‚åº¦ï¼ˆvs O(N)æš´åŠ›ï¼‰
- è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆ95-99%å‡†ç¡®ç‡ï¼‰
- int8é‡åŒ–ï¼ˆå†…å­˜-75%ï¼‰

---

#### 4. TTLCache (100-1000å€ç¼“å­˜å‘½ä¸­)
```python
# å‰: æ— é™åˆ¶å­—å…¸
self.search_cache = {}  # æ— å¤§å°é™åˆ¶ï¼Œæ— è¿‡æœŸæ—¶é—´

# å: TTLCache
from cachetools import TTLCache
self.search_cache = TTLCache(
    maxsize=10000,      # æœ€å¤š10000é¡¹
    ttl=300             # 5åˆ†é’Ÿè‡ªåŠ¨è¿‡æœŸ
)
```

**ä¼˜åŠ¿**:
- è‡ªåŠ¨è¿‡æœŸï¼ˆæ— éœ€æ‰‹åŠ¨æ¸…ç†ï¼‰
- å†…å­˜é™åˆ¶ï¼ˆé˜²æ­¢OOMï¼‰
- ç¼“å­˜å‘½ä¸­ï¼š100-1000å€åŠ é€Ÿ

---

## ğŸ” åŠŸèƒ½å¯¹æ¯”çŸ©é˜µ

| åŠŸèƒ½ | Crawler-System | HIDRS | è¯´æ˜ |
|------|:--------------:|:-----:|------|
| **æ ¸å¿ƒåŠŸèƒ½** |
| å¤šå¹³å°çˆ¬è™« | âœ… 10+ | âœ… 10+ | Wikipedia, Zhihu, Bilibiliç­‰ |
| å¹¶å‘æ§åˆ¶ | å›ºå®š5çº¿ç¨‹ | åŠ¨æ€é˜Ÿåˆ—+é™æµ | HIDRSæ”¯æŒå¯é…ç½®å¹¶å‘ |
| æ•°æ®å­˜å‚¨ | JSONæ–‡ä»¶ | MongoDB+ES | HIDRSä¼ä¸šçº§å­˜å‚¨ |
| NLPåˆ†æ | TF-IDF+K-Means | å¤šå±‚å¤„ç† | HIDRSå¢å¼ºç‰¹å¾æå– |
| **é«˜çº§åŠŸèƒ½** |
| å‘é‡æœç´¢ | âŒ | âœ… HNSW | HIDRSç‹¬æœ‰ï¼Œ768ç»´å‘é‡ |
| å®æ—¶æµå¤„ç† | âŒ | âœ… Kafka | HIDRSå®æ—¶æ•°æ®ç®¡é“ |
| å›¾åˆ†æ | âŒ | âœ… ç½‘ç»œæ‹“æ‰‘ | HIDRSç¤¾åŒºå‘ç°ç®—æ³• |
| æ–‡ä»¶åˆ†æ | åŸºç¡€HTML | âœ… Officeæ–‡æ¡£ | HIDRSæ”¯æŒPPTX/XLSX/PDF |
| é™æµå™¨ | å›ºå®šå»¶è¿Ÿ | âœ… ä»¤ç‰Œæ¡¶ | HIDRSåŠ¨æ€é™æµ |
| **UI/UX** |
| Webç•Œé¢ | âœ… ç°ä»£åŒ– | âœ… ç°ä»£åŒ– | ä¸¤è€…éƒ½æœ‰ |
| ä¸»é¢˜ç³»ç»Ÿ | âœ… 5ä¸ªé¢„è®¾ | éƒ¨åˆ† | crawler-systemæ›´ä¸°å¯Œ |
| Live2DåŠ©æ‰‹ | âœ… | âŒ | crawler-systemç‹¬æœ‰ |
| æ•°æ®å¯è§†åŒ– | âœ… è¯äº‘/ç½‘ç»œå›¾ | âœ… ECharts | ä¸¤è€…éƒ½æ”¯æŒ |
| **è¿ç»´** |
| éƒ¨ç½²å¤æ‚åº¦ | ç®€å•ï¼ˆå•å®¹å™¨ï¼‰ | å¤æ‚ï¼ˆå¤šç»„ä»¶ï¼‰ | |
| æ‰©å±•æ€§ | å‚ç›´ | æ°´å¹³ | HIDRSå¯å¤šèŠ‚ç‚¹ |
| ç›‘æ§ | åŸºç¡€ | é«˜çº§ | HIDRSæœ‰å¥åº·æ£€æŸ¥ |

---

## ğŸ’¡ ä» Crawler-System å¯ä»¥å€Ÿé‰´çš„ç‰¹æ€§

è™½ç„¶HIDRSåœ¨æ€§èƒ½å’Œæ¶æ„ä¸Šå·²ç»å…¨é¢è¶…è¶Šï¼Œä½†crawler-systemä»æœ‰ä¸€äº›å€¼å¾—å€Ÿé‰´çš„UI/UXç‰¹æ€§ï¼š

### 1. **ä¸»é¢˜ç³»ç»Ÿ** â­â­â­â­â­
```javascript
// crawler-systemçš„ä¸»é¢˜ç®¡ç† (frontend/js/theme.js)
ä¸»é¢˜åˆ—è¡¨:
1. default      - é»˜è®¤ä¸»é¢˜
2. dark         - æš—é»‘æ¨¡å¼
3. blue         - è“è‰²ä¸»é¢˜
4. green        - ç»¿è‰²ä¸»é¢˜
5. purple       - ç´«è‰²ä¸»é¢˜

ç‰¹æ€§:
- localStorageæŒä¹…åŒ–
- å¹³æ»‘è¿‡æ¸¡åŠ¨ç”»
- è‡ªå®šä¹‰èƒŒæ™¯å›¾ç‰‡
- CSSå˜é‡åŠ¨æ€åˆ‡æ¢
```

**å»ºè®®**: å°†ä¸»é¢˜ç³»ç»Ÿç§»æ¤åˆ°HIDRSçš„ `static/css/` ç›®å½•

---

### 2. **Live2DåŠ¨ç”»åŠ©æ‰‹** â­â­â­
```javascript
// crawler-systemçš„Live2Dé›†æˆ (frontend/js/live2d-manager.js)
åŠŸèƒ½:
- å¯çˆ±çš„è™šæ‹ŸåŠ©æ‰‹ï¼ˆçœ‹æ¿å¨˜ï¼‰
- é¼ æ ‡è·Ÿéš
- å¯¹è¯æ°”æ³¡
- å¯é…ç½®æ¨¡å‹
- ç‚¹å‡»äº’åŠ¨

æ¨¡å‹åˆ—è¡¨:
- Haru/haru01 (é»˜è®¤)
- Haru/haru02
- å…¶ä»–å¯æ‰©å±•æ¨¡å‹
```

**å»ºè®®**: å¯é€‰åŠŸèƒ½ï¼Œä½œä¸ºç”¨æˆ·ä½“éªŒå¢å¼ºï¼ˆä¸å½±å“æ ¸å¿ƒæ€§èƒ½ï¼‰

---

### 3. **ä»»åŠ¡è¿›åº¦å¯è§†åŒ–** â­â­â­â­
```javascript
// crawler-systemçš„å®æ—¶è¿›åº¦æ¡ (frontend/script.js)
æ˜¾ç¤ºä¿¡æ¯:
- å½“å‰çŠ¶æ€ (è¿è¡Œä¸­/å·²å®Œæˆ/å¤±è´¥)
- å·²çˆ¬å–é¡µé¢æ•°
- è¿›åº¦ç™¾åˆ†æ¯”
- é¢„è®¡å‰©ä½™æ—¶é—´
- é”™è¯¯ä¿¡æ¯

æ›´æ–°é¢‘ç‡: æ¯ç§’è½®è¯¢ä¸€æ¬¡ (/api/status/<task_id>)
```

**å»ºè®®**: HIDRSå¯ä»¥å¢å¼ºWebSocketå®æ—¶æ¨é€ï¼ˆå‡å°‘è½®è¯¢å¼€é”€ï¼‰

---

### 4. **ç»´åŸºç™¾ç§‘è·¯å¾„æŸ¥æ‰¾** â­â­â­â­
```python
# crawler-systemçš„BFSè·¯å¾„æŸ¥æ‰¾ç®—æ³• (backend/crawler.py:1019-1129)
def find_path(start_title, end_title, language='zh'):
    """
    BFSç®—æ³•æŸ¥æ‰¾ä¸¤ä¸ªç»´åŸºç™¾ç§‘æ¡ç›®ä¹‹é—´çš„æœ€çŸ­è·¯å¾„

    é™åˆ¶:
    - æœ€å¤š100é¡µï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
    - 60ç§’è¶…æ—¶
    - åŒå‘BFSï¼ˆä»èµ·ç‚¹å’Œç»ˆç‚¹åŒæ—¶æœç´¢ï¼‰
    """
    queue = deque([(start_title, [start_title])])
    visited = {start_title}

    while queue and len(visited) < 100:
        current, path = queue.popleft()

        # è·å–å½“å‰é¡µé¢çš„æ‰€æœ‰é“¾æ¥
        links = get_wikipedia_links(current, language)

        for link in links:
            if link == end_title:
                return path + [link]  # æ‰¾åˆ°è·¯å¾„

            if link not in visited:
                visited.add(link)
                queue.append((link, path + [link]))

    return None  # æœªæ‰¾åˆ°è·¯å¾„
```

**å»ºè®®**: HIDRSå·²æœ‰å›¾åˆ†ææ¨¡å—ï¼Œå¯ä»¥å¢å¼ºä¸ºé€šç”¨çš„å›¾è·¯å¾„æŸ¥æ‰¾API

---

### 5. **é˜´è°‹è®ºæ£€æµ‹** â­â­â­
```python
# crawler-systemçš„é˜´è°‹è®ºåˆ†æå™¨ (backend/crawler.py:3211-3468)
class UrbanLegendAnalyzer:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­çš„é˜´è°‹è®ºå†…å®¹

    æ–¹æ³•:
    1. å…³é”®è¯åŒ¹é…ï¼ˆå…‰æ˜ä¼šã€å¤–æ˜Ÿäººã€å…±æµä¼šç­‰ï¼‰
    2. é¢‘ç‡ç»Ÿè®¡
    3. ç½®ä¿¡åº¦è¯„åˆ†
    """

    KEYWORDS = [
        'å…‰æ˜ä¼š', 'Illuminati', 'å¤–æ˜Ÿäºº', 'å…±æµä¼š',
        'æ–°ä¸–ç•Œç§©åº', 'é˜´è°‹è®º', 'æ´—è„‘', 'æ“æ§'
    ]

    def analyze(self, text):
        score = 0
        for keyword in self.KEYWORDS:
            if keyword in text:
                score += text.count(keyword)

        return {
            'is_conspiracy': score > 5,
            'confidence': min(score / 10, 1.0),
            'keywords_found': ...
        }
```

**å»ºè®®**: å¯ä»¥é›†æˆåˆ°HIDRSçš„æ–‡æœ¬åˆ†ææ¨¡å—ï¼Œæ‰©å±•ä¸ºé€šç”¨çš„å†…å®¹åˆ†ç±»å™¨

---

### 6. **æ‰¹é‡æ“ä½œ** â­â­â­â­
```javascript
// crawler-systemçš„æ‰¹é‡æ“ä½œ (frontend/script.js:450-520)
æ”¯æŒæ‰¹é‡:
- æ‰¹é‡åˆ é™¤ä»»åŠ¡ï¼ˆæœ€å¤š20ä¸ªï¼‰
- æ‰¹é‡å¯¼å‡ºç»“æœ
- æ‰¹é‡å–æ¶ˆä»»åŠ¡
- æ‰¹é‡é‡è¯•å¤±è´¥ä»»åŠ¡

UIç‰¹æ€§:
- å…¨é€‰/åé€‰
- é€‰ä¸­è®¡æ•°å™¨
- æ“ä½œç¡®è®¤å¯¹è¯æ¡†
- è¿›åº¦åé¦ˆ
```

**å»ºè®®**: HIDRSå¯ä»¥å¢å¼ºæ‰¹é‡APIï¼ˆMongoDBçš„bulkWriteï¼‰

---

## ğŸ¯ HIDRSè¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®

åŸºäºå¯¹crawler-systemçš„åˆ†æå’ŒHIDRSç°çŠ¶ï¼Œæå‡ºä»¥ä¸‹ä¼˜åŒ–å»ºè®®ï¼š

### çŸ­æœŸä¼˜åŒ– (1-2å‘¨)

#### 1. **ç§»æ¤ä¸»é¢˜ç³»ç»Ÿ** â­â­â­â­â­
```bash
ä»»åŠ¡: å°†crawler-systemçš„ä¸»é¢˜ç®¡ç†ç§»æ¤åˆ°HIDRS
å·¥ä½œé‡: 4-8å°æ—¶
ä¼˜å…ˆçº§: é«˜ï¼ˆç”¨æˆ·ä½“éªŒæå‡ï¼‰

æ­¥éª¤:
1. å¤åˆ¶ frontend/css/themes.css â†’ hidrs/static/css/
2. å¤åˆ¶ frontend/js/theme.js â†’ hidrs/static/js/
3. åœ¨ index.html ä¸­é›†æˆä¸»é¢˜åˆ‡æ¢å™¨
4. æµ‹è¯•5ä¸ªé¢„è®¾ä¸»é¢˜
```

#### 2. **WebSocketå®æ—¶æ¨é€** â­â­â­â­
```python
# æ›¿æ¢HTTPè½®è¯¢ä¸ºWebSocket
from flask_socketio import SocketIO, emit

socketio = SocketIO(app)

@socketio.on('subscribe_task')
def handle_task_subscription(task_id):
    """å®¢æˆ·ç«¯è®¢é˜…ä»»åŠ¡è¿›åº¦"""
    room = f'task_{task_id}'
    join_room(room)
    emit('subscribed', {'task_id': task_id})

def update_task_progress(task_id, progress):
    """æœåŠ¡ç«¯æ¨é€è¿›åº¦æ›´æ–°"""
    socketio.emit('task_progress', {
        'task_id': task_id,
        'progress': progress,
        'status': 'running'
    }, room=f'task_{task_id}')
```

**ä¼˜åŠ¿**:
- å‡å°‘HTTPè½®è¯¢å¼€é”€ï¼ˆä»æ¯ç§’1æ¬¡åˆ°å®æ—¶æ¨é€ï¼‰
- é™ä½æœåŠ¡å™¨è´Ÿè½½
- æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ

---

### ä¸­æœŸä¼˜åŒ– (1-2ä¸ªæœˆ)

#### 3. **å›¾è·¯å¾„æŸ¥æ‰¾API** â­â­â­â­
```python
# åŸºäºNetworkXçš„é€šç”¨è·¯å¾„æŸ¥æ‰¾
import networkx as nx

class GraphPathFinder:
    def __init__(self, mongodb_uri):
        self.graph = nx.Graph()
        self.db = MongoClient(mongodb_uri)['hidrs_db']

    def build_graph_from_crawl_data(self):
        """ä»çˆ¬è™«æ•°æ®æ„å»ºå›¾"""
        for doc in self.db.raw_data_collection.find():
            # æå–é“¾æ¥å…³ç³»
            source = doc['url']
            for link in doc.get('links', []):
                self.graph.add_edge(source, link)

    def find_shortest_path(self, start, end):
        """BFSæœ€çŸ­è·¯å¾„"""
        try:
            path = nx.shortest_path(self.graph, start, end)
            return {
                'path': path,
                'length': len(path) - 1,
                'found': True
            }
        except nx.NetworkXNoPath:
            return {'found': False}

    def find_all_paths(self, start, end, max_length=6):
        """æ‰€æœ‰è·¯å¾„ï¼ˆé™åˆ¶æœ€å¤§é•¿åº¦ï¼‰"""
        paths = nx.all_simple_paths(
            self.graph, start, end,
            cutoff=max_length
        )
        return list(paths)
```

**æ–°å¢APIç«¯ç‚¹**:
```
POST /api/graph/shortest-path
{
    "start": "https://example.com/A",
    "end": "https://example.com/B"
}

â†’ {
    "path": ["A", "C", "D", "B"],
    "length": 3
}
```

---

#### 4. **å†…å®¹åˆ†ç±»å™¨** â­â­â­â­
```python
# æ‰©å±•é˜´è°‹è®ºæ£€æµ‹ä¸ºé€šç”¨åˆ†ç±»å™¨
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer

class ContentClassifier:
    """é€šç”¨æ–‡æœ¬åˆ†ç±»å™¨"""

    CATEGORIES = {
        'conspiracy': ['å…‰æ˜ä¼š', 'å¤–æ˜Ÿäºº', 'å…±æµä¼š', 'é˜´è°‹'],
        'fake_news': ['å‡æ–°é—»', 'æœªç»è¯å®', 'ä¼ è¨€', 'è°£è¨€'],
        'spam': ['å¹¿å‘Š', 'æ¨å¹¿', 'ä¼˜æƒ ', 'ç‚¹å‡»'],
        'political': ['æ”¿æ²»', 'é€‰ä¸¾', 'æ”¿åºœ', 'æ”¿ç­–'],
        'tech': ['æŠ€æœ¯', 'ç§‘æŠ€', 'äººå·¥æ™ºèƒ½', 'åŒºå—é“¾']
    }

    def __init__(self):
        self.vectorizer = TfidfVectorizer(max_features=1000)
        self.classifier = MultinomialNB()

    def train(self, texts, labels):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        X = self.vectorizer.fit_transform(texts)
        self.classifier.fit(X, labels)

    def predict(self, text):
        """é¢„æµ‹åˆ†ç±»"""
        X = self.vectorizer.transform([text])
        probas = self.classifier.predict_proba(X)[0]

        return {
            'category': self.classifier.classes_[probas.argmax()],
            'confidence': float(probas.max()),
            'probabilities': dict(zip(self.classifier.classes_, probas))
        }
```

**é›†æˆåˆ°çˆ¬è™«**:
```python
# åœ¨æ•°æ®å­˜å‚¨æ—¶è‡ªåŠ¨åˆ†ç±»
def store_crawl_result(url, content):
    classification = content_classifier.predict(content)

    document = {
        'url': url,
        'content': content,
        'classification': classification,
        'timestamp': datetime.now()
    }

    db.raw_data_collection.insert_one(document)
```

---

#### 5. **æ‰¹é‡æ“ä½œAPI** â­â­â­â­
```python
# MongoDB bulkWriteä¼˜åŒ–
from pymongo import UpdateOne, DeleteOne

@app.route('/api/tasks/batch-delete', methods=['POST'])
def batch_delete_tasks():
    """æ‰¹é‡åˆ é™¤ä»»åŠ¡ï¼ˆæœ€å¤š100ä¸ªï¼‰"""
    task_ids = request.json.get('task_ids', [])

    if len(task_ids) > 100:
        return jsonify({'error': 'Maximum 100 tasks per batch'}), 400

    # bulkWriteæ‰¹é‡æ“ä½œï¼ˆæ¯”å¾ªç¯å¿«10-100å€ï¼‰
    operations = [
        DeleteOne({'_id': ObjectId(task_id)})
        for task_id in task_ids
    ]

    result = db.tasks_collection.bulk_write(operations)

    return jsonify({
        'deleted_count': result.deleted_count,
        'success': True
    })

@app.route('/api/tasks/batch-retry', methods=['POST'])
def batch_retry_tasks():
    """æ‰¹é‡é‡è¯•å¤±è´¥ä»»åŠ¡"""
    task_ids = request.json.get('task_ids', [])

    operations = [
        UpdateOne(
            {'_id': ObjectId(task_id)},
            {'$set': {
                'status': 'pending',
                'retry_count': {'$inc': 1},
                'updated_at': datetime.now()
            }}
        )
        for task_id in task_ids
    ]

    result = db.tasks_collection.bulk_write(operations)

    return jsonify({
        'modified_count': result.modified_count,
        'success': True
    })
```

---

### é•¿æœŸä¼˜åŒ– (3-6ä¸ªæœˆ)

#### 6. **Live2Dé›†æˆ** â­â­â­
```javascript
// å¯é€‰çš„ç”¨æˆ·ä½“éªŒå¢å¼º
// ä¸å½±å“æ ¸å¿ƒæ€§èƒ½ï¼Œçº¯å‰ç«¯å®ç°

<!-- å¼•å…¥Live2Dåº“ -->
<script src="https://cdn.jsdelivr.net/npm/live2d-widget@3.1.4/lib/L2Dwidget.min.js"></script>

<script>
  L2Dwidget.init({
    model: {
      jsonPath: '/static/live2d/haru/haru01.model.json'
    },
    display: {
      position: 'right',
      width: 150,
      height: 300
    },
    mobile: {
      show: false  // ç§»åŠ¨ç«¯ä¸æ˜¾ç¤º
    }
  });
</script>
```

**åŠŸèƒ½**:
- è™šæ‹ŸåŠ©æ‰‹ï¼ˆçœ‹æ¿å¨˜ï¼‰
- é¼ æ ‡è·Ÿéš
- å¯¹è¯æ°”æ³¡ï¼ˆæ˜¾ç¤ºç³»ç»Ÿæç¤ºï¼‰
- ç‚¹å‡»äº’åŠ¨

**æ³¨æ„**: çº¯å‰ç«¯å®ç°ï¼Œä¸å¢åŠ æœåŠ¡å™¨è´Ÿæ‹…

---

#### 7. **æŸ¥è¯¢æ„å›¾è¯†åˆ«** â­â­â­â­
```python
# ä½¿ç”¨æœºå™¨å­¦ä¹ è¯†åˆ«ç”¨æˆ·æŸ¥è¯¢æ„å›¾
from transformers import pipeline

class QueryIntentClassifier:
    """æŸ¥è¯¢æ„å›¾è¯†åˆ«å™¨"""

    INTENTS = {
        'search': 'æ™®é€šæœç´¢',
        'statistics': 'ç»Ÿè®¡æŸ¥è¯¢',
        'pathfinding': 'è·¯å¾„æŸ¥æ‰¾',
        'analysis': 'æ·±åº¦åˆ†æ'
    }

    def __init__(self):
        self.classifier = pipeline(
            'text-classification',
            model='bert-base-chinese'
        )

    def classify(self, query):
        """è¯†åˆ«æŸ¥è¯¢æ„å›¾"""
        result = self.classifier(query)[0]

        return {
            'intent': result['label'],
            'confidence': result['score']
        }

    def route_query(self, query):
        """æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒæœåŠ¡"""
        intent = self.classify(query)

        if intent['intent'] == 'search':
            return search_engine.search(query)
        elif intent['intent'] == 'statistics':
            return stats_engine.get_stats(query)
        elif intent['intent'] == 'pathfinding':
            return graph_finder.find_path(query)
        else:
            return analysis_engine.analyze(query)
```

---

## ğŸ“‹ ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–é¡¹ | éš¾åº¦ | æ”¶ç›Š | ä¼˜å…ˆçº§ | å·¥ä½œé‡ |
|--------|------|------|--------|--------|
| ä¸»é¢˜ç³»ç»Ÿç§»æ¤ | ä½ | é«˜ | â­â­â­â­â­ | 4-8h |
| WebSocketæ¨é€ | ä¸­ | é«˜ | â­â­â­â­â­ | 16-24h |
| æ‰¹é‡æ“ä½œAPI | ä½ | é«˜ | â­â­â­â­ | 8-12h |
| å›¾è·¯å¾„æŸ¥æ‰¾ | ä¸­ | ä¸­ | â­â­â­â­ | 24-40h |
| å†…å®¹åˆ†ç±»å™¨ | ä¸­ | ä¸­ | â­â­â­ | 32-48h |
| Live2Dé›†æˆ | ä½ | ä½ | â­â­ | 8-12h |
| æŸ¥è¯¢æ„å›¾è¯†åˆ« | é«˜ | é«˜ | â­â­â­â­ | 80-120h |

---

## ğŸ¯ å®æ–½å»ºè®®

### ç¬¬ä¸€é˜¶æ®µ (æœ¬å‘¨)
1. âœ… **ä¸»é¢˜ç³»ç»Ÿç§»æ¤** - æå‡UI/UX
2. âœ… **æ‰¹é‡æ“ä½œAPI** - æå‡ç®¡ç†æ•ˆç‡

### ç¬¬äºŒé˜¶æ®µ (ä¸‹å‘¨)
3. **WebSocketæ¨é€** - å‡å°‘è½®è¯¢å¼€é”€
4. **å›¾è·¯å¾„æŸ¥æ‰¾** - å¢å¼ºå›¾åˆ†æèƒ½åŠ›

### ç¬¬ä¸‰é˜¶æ®µ (1ä¸ªæœˆå†…)
5. **å†…å®¹åˆ†ç±»å™¨** - è‡ªåŠ¨å†…å®¹è¯†åˆ«
6. **æŸ¥è¯¢æ„å›¾è¯†åˆ«** - æ™ºèƒ½è·¯ç”±

### ç¬¬å››é˜¶æ®µ (å¯é€‰)
7. **Live2Dé›†æˆ** - ç”¨æˆ·ä½“éªŒå¢å¼º

---

## ğŸ”„ æŠ€æœ¯å€ºåŠ¡æ¸…ç†

ä»crawler-systemè¿ç§»åˆ°HIDRSè¿‡ç¨‹ä¸­éœ€è¦æ¸…ç†çš„æŠ€æœ¯å€ºåŠ¡ï¼š

### 1. **å•ä½“æ–‡ä»¶æ‹†åˆ†**
```
crawler.py (~4300è¡Œ) â†’ æ‹†åˆ†ä¸º:
  â”œâ”€â”€ crawlers/
  â”‚   â”œâ”€â”€ base_crawler.py
  â”‚   â”œâ”€â”€ wikipedia_crawler.py
  â”‚   â”œâ”€â”€ zhihu_crawler.py
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ processors/
  â”‚   â”œâ”€â”€ data_processor.py
  â”‚   â”œâ”€â”€ nlp_analyzer.py
  â”‚   â””â”€â”€ ...
  â””â”€â”€ utils/
      â”œâ”€â”€ storage_manager.py
      â””â”€â”€ ...
```

### 2. **é…ç½®ç®¡ç†**
```python
# å‰: ç¡¬ç¼–ç é…ç½®
MAX_RETRIES = 3
TIMEOUT = 30

# å: ç¯å¢ƒå˜é‡ + é…ç½®æ–‡ä»¶
import os
from dotenv import load_load_env()

MAX_RETRIES = int(os.getenv('MAX_RETRIES', 3))
TIMEOUT = int(os.getenv('TIMEOUT', 30))
```

### 3. **æ—¥å¿—ç³»ç»Ÿ**
```python
# å‰: printè¯­å¥
print(f"Crawling {url}...")

# å: ç»“æ„åŒ–æ—¥å¿—
import logging
logger = logging.getLogger(__name__)
logger.info("Crawling URL", extra={'url': url, 'timestamp': time.time()})
```

---

## ğŸ“Š æ€»ç»“

### Crawler-Systemçš„ä»·å€¼
1. âœ… **ä¼˜ç§€çš„UI/UXè®¾è®¡** - ä¸»é¢˜ç³»ç»Ÿã€Live2Dã€å¯è§†åŒ–
2. âœ… **å®Œæ•´çš„åŠŸèƒ½é›†** - 10+å¹³å°çˆ¬è™«ã€NLPåˆ†æã€è·¯å¾„æŸ¥æ‰¾
3. âœ… **æ˜“äºéƒ¨ç½²** - å•å®¹å™¨ã€é›¶é…ç½®
4. âœ… **é€‚åˆåŸå‹å¼€å‘** - å¿«é€ŸéªŒè¯æƒ³æ³•

### HIDRSçš„ä¼˜åŠ¿
1. âœ… **ä¼ä¸šçº§æ€§èƒ½** - 10-100å€é€Ÿåº¦æå‡
2. âœ… **åˆ†å¸ƒå¼æ¶æ„** - æ¨ªå‘æ‰©å±•èƒ½åŠ›
3. âœ… **é«˜çº§åŠŸèƒ½** - å‘é‡æœç´¢ã€å®æ—¶æµã€å›¾åˆ†æ
4. âœ… **ç”Ÿäº§å°±ç»ª** - ç›‘æ§ã€é™æµã€å®¹é”™

### æœ€ä½³å®è·µ
- **crawler-system**: é€‚åˆä¸ªäººé¡¹ç›®ã€å¿«é€ŸåŸå‹ã€æ•™å­¦æ¼”ç¤º
- **HIDRS**: é€‚åˆç”Ÿäº§ç¯å¢ƒã€å¤§è§„æ¨¡éƒ¨ç½²ã€ä¼ä¸šåº”ç”¨

### å»ºè®®
å°†crawler-systemçš„UI/UXç‰¹æ€§ï¼ˆä¸»é¢˜ã€Live2Dã€æ‰¹é‡æ“ä½œï¼‰ç§»æ¤åˆ°HIDRSï¼Œç»“åˆä¸¤è€…ä¼˜åŠ¿æ‰“é€ å®Œç¾çš„çˆ¬è™«ç³»ç»Ÿã€‚

---

**æŠ¥å‘Šå®Œæˆæ—¥æœŸ**: 2026-02-04
**åˆ†ææ·±åº¦**: å®Œæ•´æ¶æ„ã€æ€§èƒ½ã€åŠŸèƒ½å¯¹æ¯”
**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**: å‚è€ƒ"ä¼˜å…ˆçº§çŸ©é˜µ"é€æ­¥å®æ–½ä¼˜åŒ–
