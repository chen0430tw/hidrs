# HIDRSæœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶è§„åˆ’

## 1. æ¦‚è¿°

### 1.1 ç›®æ ‡

è®¾è®¡ä¸€ä¸ªæœ¬åœ°æ–‡ä»¶æœç´¢ä¸åˆ†ææ’ä»¶ï¼Œç”¨äºï¼š
1. **å¿«é€Ÿæ–‡ä»¶ç´¢å¼•ä¸æœç´¢** - ç§’çº§å“åº”ï¼Œæ”¯æŒæµ·é‡æ–‡ä»¶
2. **ç£ç›˜ç©ºé—´"çƒ­èƒ€å†·ç¼©"åˆ†æ** - è¿½è¸ªæ¸…ç†åç©ºé—´å¿«é€Ÿå›å¡«çš„åŸå› 
3. **æ–‡ä»¶å…³è”æ€§åˆ†æ** - ç»“åˆHIDRSæ‹‰æ™®æ‹‰æ–¯è°±åˆ†æï¼Œå‘ç°æ–‡ä»¶ä¹‹é—´çš„éšè—å…³è”
4. **é‡å¤æ–‡ä»¶æ£€æµ‹** - è¯†åˆ«å ç”¨å¤§é‡ç©ºé—´çš„é‡å¤å†…å®¹
5. **æ—¶é—´çº¿è¿½è¸ª** - ç›‘æ§æ–‡ä»¶å¢é•¿è¶‹åŠ¿ï¼Œé¢„è­¦å¼‚å¸¸ç©ºé—´å ç”¨

### 1.2 åº”ç”¨åœºæ™¯

- **é—®é¢˜è¯Šæ–­**: åˆšæ¸…å‡º7GBå®¹é‡ï¼Œé©¬ä¸Šè¢«å›å¡«5GB - æ‰¾å‡ºç½ªé­ç¥¸é¦–
- **ç©ºé—´ä¼˜åŒ–**: è¯†åˆ«å¯å®‰å…¨åˆ é™¤çš„ä¸´æ—¶æ–‡ä»¶ã€ç¼“å­˜ã€é‡å¤æ–‡ä»¶
- **å–è¯åˆ†æ**: å‘ç°éšè—çš„å¤§æ–‡ä»¶ã€å¯ç–‘çš„å¿«é€Ÿå¢é•¿æ–‡ä»¶
- **æ•°æ®æ¢å¤**: ç»“åˆæ–‡ä»¶ç­¾åè¯†åˆ«ï¼Œæ¢å¤è¯¯åˆ æ–‡ä»¶

---

## 2. ç°æœ‰å·¥å…·è°ƒç ”ï¼ˆ2026ï¼‰

### 2.1 Everything - æé€Ÿæ–‡ä»¶åæœç´¢

**æ ¸å¿ƒæŠ€æœ¯**:
- ç›´æ¥è¯»å–NTFS MFTï¼ˆä¸»æ–‡ä»¶è¡¨ï¼‰
- ä½¿ç”¨USNå˜æ›´æ—¥å¿—å®æ—¶ç›‘æ§æ–‡ä»¶å˜åŒ–
- ç§’çº§ç´¢å¼•æ•´ä¸ªç³»ç»Ÿï¼Œæ”¯æŒæ•°åä¸‡æ–‡ä»¶

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… å¸ƒå°”è¿ç®—ç¬¦ (AND, OR, NOT)
- âœ… æ­£åˆ™è¡¨è¾¾å¼æ”¯æŒ
- âœ… å®æ—¶æ›´æ–°ï¼ˆæ— éœ€é‡æ–°æ‰«æï¼‰
- âœ… æä½èµ„æºå ç”¨ï¼ˆæ•°æ®åº“ä»…æ•°MBï¼‰
- âœ… v1.5+ æ”¯æŒå†…å®¹ç´¢å¼•ï¼ˆå¯é€‰ï¼‰

**å±€é™æ€§**:
- âŒ ä»…æ”¯æŒWindows NTFS
- âŒ ä¸æ”¯æŒç£ç›˜ç©ºé—´å¯è§†åŒ–
- âŒ æ— æ–‡ä»¶å…³è”æ€§åˆ†æ

**å‚è€ƒ**: [Everything - voidtools](https://www.voidtools.com/)

### 2.2 R-Studio - ä¸“ä¸šæ•°æ®æ¢å¤

**æ ¸å¿ƒæŠ€æœ¯**:
- æ–‡ä»¶ç­¾åè¯†åˆ«ï¼ˆæ”¯æŒè‡ªå®šä¹‰ç­¾åï¼‰
- åå…­è¿›åˆ¶æ¨¡å¼æœç´¢
- æ·±åº¦æ‰«ææŸåçš„æ–‡ä»¶ç³»ç»Ÿ

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… åŸå§‹æ–‡ä»¶æœç´¢ï¼ˆæŒ‰æ–‡ä»¶ç±»å‹ï¼‰
- âœ… æ­£åˆ™è¡¨è¾¾å¼ + åå…­è¿›åˆ¶æ¨¡å¼
- âœ… è‡ªå®šä¹‰æ–‡ä»¶ç±»å‹å®šä¹‰
- âœ… æŸåæ–‡ä»¶ç³»ç»Ÿæ¢å¤

**å±€é™æ€§**:
- âŒ ä¸“æ³¨æ•°æ®æ¢å¤ï¼Œéæ—¥å¸¸æœç´¢å·¥å…·
- âŒ å•†ä¸šè½¯ä»¶ï¼ˆ$49.99 - $999ï¼‰
- âŒ æ— ç©ºé—´åˆ†æåŠŸèƒ½

**å‚è€ƒ**: [R-Studio Data Recovery Software](https://www.r-studio.com/)

### 2.3 ç£ç›˜ç©ºé—´åˆ†æå·¥å…·

#### WizTree - æœ€å¿«çš„ç©ºé—´åˆ†æå™¨

**æ ¸å¿ƒæŠ€æœ¯**:
- ç›´æ¥è¯»å–MFTï¼ˆä¸EverythingåŒåŸç†ï¼‰
- æ•°ç§’å†…æ‰«æTBçº§ç¡¬ç›˜

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… æ ‘å½¢å›¾ï¼ˆTreemapï¼‰å¯è§†åŒ–
- âœ… æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„
- âœ… å¯¼å‡ºCSVæŠ¥å‘Š
- âœ… å…è´¹ä¸ªäººä½¿ç”¨

**å‚è€ƒ**: [WizTree - The Fastest Disk Space Analyzer](https://diskanalyzer.com/)

#### TreeSize - åŠŸèƒ½ä¸°å¯Œçš„ä¼ä¸šçº§å·¥å…·

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… å¤šç§å¯è§†åŒ–æ–¹å¼ï¼ˆæ ‘çŠ¶ã€å›¾è¡¨ã€Treemapï¼‰
- âœ… è‡ªå®šä¹‰æ–‡ä»¶æœç´¢
- âœ… è¯¦ç»†æŠ¥å‘Šå¯¼å‡º
- âœ… æ–‡ä»¶å¹´é¾„åˆ†æï¼ˆFile Age Viewï¼‰
- âœ… é‡å¤æ–‡ä»¶æŸ¥æ‰¾

**ç‰ˆæœ¬**: Free / Personal / Professional

**å‚è€ƒ**: [TreeSize â€“ Official Free Download](https://www.jam-software.com/treesize)

#### WinDirStat - å¼€æºç»å…¸

**åŠŸèƒ½ç‰¹æ€§**:
- âœ… ç»å…¸çš„Treemapå¯è§†åŒ–
- âœ… å®Œå…¨å¼€æºå…è´¹
- âœ… æ‰©å±•åç»Ÿè®¡
- âœ… æ–‡ä»¶ç±»å‹æ¸…ç†åˆ—è¡¨

**å±€é™æ€§**:
- âŒ æ‰«æé€Ÿåº¦è¾ƒæ…¢ï¼ˆä¸ä½¿ç”¨MFTç›´æ¥è¯»å–ï¼‰

**å‚è€ƒ**: [WinDirStat - Windows Directory Statistics](https://windirstat.net/)

---

## 3. HIDRSæœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶è®¾è®¡

### 3.1 æ¶æ„è®¾è®¡

```
LocalFileSearchPlugin
â”œâ”€â”€ ç´¢å¼•å¼•æ“ (IndexEngine)
â”‚   â”œâ”€â”€ MFTReader (Windows NTFSç›´æ¥è¯»å–)
â”‚   â”œâ”€â”€ FileSystemWatcher (Linux/Mac/å…¶ä»–FS)
â”‚   â””â”€â”€ IncrementalIndexer (å¢é‡æ›´æ–°)
â”‚
â”œâ”€â”€ æœç´¢å¼•æ“ (SearchEngine)
â”‚   â”œâ”€â”€ NameSearch (æ–‡ä»¶åæ­£åˆ™æœç´¢)
â”‚   â”œâ”€â”€ ContentSearch (å†…å®¹å…¨æ–‡ç´¢å¼•)
â”‚   â””â”€â”€ SignatureSearch (æ–‡ä»¶ç­¾åè¯†åˆ«)
â”‚
â”œâ”€â”€ ç©ºé—´åˆ†æå™¨ (SpaceAnalyzer)
â”‚   â”œâ”€â”€ TreemapGenerator (æ ‘å½¢å›¾ç”Ÿæˆ)
â”‚   â”œâ”€â”€ FileTypeStatistics (æ–‡ä»¶ç±»å‹ç»Ÿè®¡)
â”‚   â””â”€â”€ TimelineTracker (æ—¶é—´çº¿è¿½è¸ª)
â”‚
â”œâ”€â”€ å…³è”åˆ†æå™¨ (RelationAnalyzer)
â”‚   â”œâ”€â”€ LaplacianMapper (æ–‡ä»¶æ‹‰æ™®æ‹‰æ–¯å‘é‡)
â”‚   â”œâ”€â”€ DuplicateFinder (é‡å¤æ–‡ä»¶æ£€æµ‹)
â”‚   â””â”€â”€ ClusterAnalyzer (æ–‡ä»¶èšç±»åˆ†æ)
â”‚
â””â”€â”€ è¯Šæ–­å¼•æ“ (DiagnosticEngine)
    â”œâ”€â”€ SpaceLeakDetector (ç©ºé—´æ³„æ¼æ£€æµ‹)
    â”œâ”€â”€ GrowthMonitor (å¢é•¿ç›‘æ§)
    â””â”€â”€ AnomalyDetector (å¼‚å¸¸æ£€æµ‹)
```

### 3.2 æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

#### 3.2.1 ç´¢å¼•å¼•æ“ (IndexEngine)

**MFT Reader (Windows)**:
```python
import win32file
import struct

class MFTReader:
    """ç›´æ¥è¯»å–NTFS MFTï¼Œç±»ä¼¼Everythingçš„å®ç°"""

    def __init__(self, drive_letter: str = 'C'):
        self.drive = f"\\\\.\\{drive_letter}:"
        self.mft_records = []

    def read_mft(self) -> List[Dict[str, Any]]:
        """è¯»å–MFTæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        handle = win32file.CreateFile(
            self.drive,
            win32file.GENERIC_READ,
            win32file.FILE_SHARE_READ | win32file.FILE_SHARE_WRITE,
            None,
            win32file.OPEN_EXISTING,
            0,
            None
        )

        # è¯»å–MFTèµ·å§‹ä½ç½®
        mft_start = self._get_mft_start(handle)

        # æŒ‰1024å­—èŠ‚å—è¯»å–MFTè®°å½•
        records = []
        offset = mft_start
        while True:
            try:
                win32file.SetFilePointer(handle, offset, win32file.FILE_BEGIN)
                data = win32file.ReadFile(handle, 1024)[1]

                if data[:4] == b'FILE':  # MFTè®°å½•ç­¾å
                    record = self._parse_mft_record(data)
                    if record:
                        records.append(record)

                offset += 1024
            except:
                break

        win32file.CloseHandle(handle)
        return records

    def _parse_mft_record(self, data: bytes) -> Optional[Dict]:
        """è§£æå•ä¸ªMFTè®°å½•"""
        # FILEè®°å½•æ ¼å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰
        # 0x00-0x03: "FILE" ç­¾å
        # 0x16-0x17: å±æ€§åç§»
        # 0x20-0x27: æ–‡ä»¶å¼•ç”¨å·

        try:
            file_reference = struct.unpack('<Q', data[0x20:0x28])[0]
            attr_offset = struct.unpack('<H', data[0x14:0x16])[0]

            # è§£æå±æ€§ï¼ˆ$FILE_NAME, $DATAç­‰ï¼‰
            attributes = self._parse_attributes(data[attr_offset:])

            return {
                'file_reference': file_reference,
                'name': attributes.get('filename'),
                'size': attributes.get('data_size', 0),
                'created': attributes.get('created'),
                'modified': attributes.get('modified'),
                'path': attributes.get('path'),
            }
        except:
            return None

    def monitor_changes(self, callback):
        """ç›‘æ§USNå˜æ›´æ—¥å¿—ï¼ˆå®æ—¶æ›´æ–°ï¼‰"""
        # ä½¿ç”¨DeviceIoControlè¯»å–USNæ—¥å¿—
        pass
```

**è·¨å¹³å°æ–‡ä»¶ç›‘æ§**:
```python
import os
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class FileChangeHandler(FileSystemEventHandler):
    """è·¨å¹³å°æ–‡ä»¶ç³»ç»Ÿç›‘æ§ï¼ˆLinux/Mac/WindowséNTFSï¼‰"""

    def __init__(self, index_manager):
        self.index = index_manager

    def on_created(self, event):
        if not event.is_directory:
            self.index.add_file(event.src_path)

    def on_deleted(self, event):
        if not event.is_directory:
            self.index.remove_file(event.src_path)

    def on_modified(self, event):
        if not event.is_directory:
            self.index.update_file(event.src_path)

    def on_moved(self, event):
        self.index.move_file(event.src_path, event.dest_path)
```

#### 3.2.2 ç©ºé—´åˆ†æå™¨ (SpaceAnalyzer)

**ç£ç›˜ç©ºé—´"çƒ­èƒ€å†·ç¼©"è¯Šæ–­**:
```python
class SpaceLeakDetector:
    """ç©ºé—´æ³„æ¼æ£€æµ‹å™¨ - æ‰¾å‡º"æ¸…ç†7GBï¼Œå›å¡«5GB"çš„åŸå› """

    def __init__(self, index_manager):
        self.index = index_manager
        self.baseline = None  # æ¸…ç†åçš„åŸºçº¿å¿«ç…§

    def create_baseline_snapshot(self):
        """åˆ›å»ºåŸºçº¿å¿«ç…§ï¼ˆæ¸…ç†åç«‹å³æ‰§è¡Œï¼‰"""
        self.baseline = {
            'timestamp': time.time(),
            'total_size': self._get_total_size(),
            'file_count': self.index.get_file_count(),
            'top_dirs': self._get_top_directories(100),
            'files_by_type': self._group_by_type(),
        }

    def detect_space_leak(self, interval_minutes: int = 60) -> Dict:
        """æ£€æµ‹ç©ºé—´æ³„æ¼ï¼ˆæ¸…ç†åNåˆ†é’Ÿå¯¹æ¯”ï¼‰"""
        if not self.baseline:
            raise ValueError("è¯·å…ˆåˆ›å»ºåŸºçº¿å¿«ç…§")

        current = {
            'timestamp': time.time(),
            'total_size': self._get_total_size(),
            'file_count': self.index.get_file_count(),
            'top_dirs': self._get_top_directories(100),
            'files_by_type': self._group_by_type(),
        }

        # è®¡ç®—å¢é‡
        delta = {
            'size_increase': current['total_size'] - self.baseline['total_size'],
            'file_increase': current['file_count'] - self.baseline['file_count'],
            'time_elapsed': (current['timestamp'] - self.baseline['timestamp']) / 60,
            'growth_rate_mb_per_hour': 0,
        }

        delta['growth_rate_mb_per_hour'] = (
            delta['size_increase'] / (1024 * 1024) / (delta['time_elapsed'] / 60)
        )

        # æ‰¾å‡ºå¿«é€Ÿå¢é•¿çš„ç›®å½•
        fast_growing_dirs = []
        for dir_path in current['top_dirs']:
            baseline_size = self.baseline['top_dirs'].get(dir_path, 0)
            current_size = current['top_dirs'][dir_path]
            growth = current_size - baseline_size

            if growth > 100 * 1024 * 1024:  # å¢é•¿è¶…è¿‡100MB
                fast_growing_dirs.append({
                    'path': dir_path,
                    'growth_mb': growth / (1024 * 1024),
                    'baseline_mb': baseline_size / (1024 * 1024),
                    'current_mb': current_size / (1024 * 1024),
                    'growth_rate': (growth / baseline_size * 100) if baseline_size > 0 else float('inf'),
                })

        # æŒ‰å¢é•¿é‡æ’åº
        fast_growing_dirs.sort(key=lambda x: x['growth_mb'], reverse=True)

        # è¯†åˆ«å¸¸è§"ç½ªé­ç¥¸é¦–"
        culprits = self._identify_culprits(fast_growing_dirs)

        return {
            'summary': delta,
            'fast_growing_dirs': fast_growing_dirs[:20],  # Top 20
            'culprits': culprits,
            'recommendations': self._generate_recommendations(culprits),
        }

    def _identify_culprits(self, growing_dirs: List[Dict]) -> List[Dict]:
        """è¯†åˆ«å¸¸è§çš„ç©ºé—´å ç”¨"ç½ªé­ç¥¸é¦–""""
        culprits = []

        # å·²çŸ¥çš„ç©ºé—´æ¶ˆè€—è·¯å¾„æ¨¡å¼
        patterns = {
            'windows_update': {
                'paths': [
                    r'C:\\Windows\\SoftwareDistribution',
                    r'C:\\Windows\\WinSxS',
                ],
                'name': 'Windowsæ›´æ–°ç¼“å­˜',
                'description': 'Windows Updateä¸‹è½½çš„æ›´æ–°åŒ…',
                'solution': 'è¿è¡Œ"ç£ç›˜æ¸…ç†" > "æ¸…ç†ç³»ç»Ÿæ–‡ä»¶" > å‹¾é€‰"Windowsæ›´æ–°æ¸…ç†"',
            },
            'system_restore': {
                'paths': [
                    r'C:\\System Volume Information',
                ],
                'name': 'ç³»ç»Ÿè¿˜åŸç‚¹/å·å½±å‰¯æœ¬',
                'description': 'Windowsè‡ªåŠ¨åˆ›å»ºçš„è¿˜åŸç‚¹',
                'solution': 'æ§åˆ¶é¢æ¿ > ç³»ç»Ÿ > ç³»ç»Ÿä¿æŠ¤ > é…ç½® > å‡å°‘ç£ç›˜ä½¿ç”¨é‡',
            },
            'temp_files': {
                'paths': [
                    r'C:\\Windows\\Temp',
                    r'C:\\Users\\.*\\AppData\\Local\\Temp',
                ],
                'name': 'ä¸´æ—¶æ–‡ä»¶',
                'description': 'ç¨‹åºè¿è¡Œæ—¶äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶',
                'solution': 'è¿è¡Œ"ç£ç›˜æ¸…ç†" > å‹¾é€‰"ä¸´æ—¶æ–‡ä»¶"',
            },
            'browser_cache': {
                'paths': [
                    r'C:\\Users\\.*\\AppData\\Local\\Google\\Chrome\\User Data',
                    r'C:\\Users\\.*\\AppData\\Local\\Microsoft\\Edge',
                    r'C:\\Users\\.*\\AppData\\Roaming\\Mozilla\\Firefox',
                ],
                'name': 'æµè§ˆå™¨ç¼“å­˜',
                'description': 'ç½‘é¡µç¼“å­˜ã€ä¸‹è½½æ–‡ä»¶ã€æ‰©å±•æ•°æ®',
                'solution': 'æµè§ˆå™¨è®¾ç½® > æ¸…é™¤æµè§ˆæ•°æ® > ç¼“å­˜å›¾åƒå’Œæ–‡ä»¶',
            },
            'pagefile': {
                'paths': [
                    r'C:\\pagefile.sys',
                    r'C:\\swapfile.sys',
                    r'C:\\hiberfil.sys',
                ],
                'name': 'è™šæ‹Ÿå†…å­˜/ä¼‘çœ æ–‡ä»¶',
                'description': 'ç³»ç»Ÿç®¡ç†çš„è™šæ‹Ÿå†…å­˜å’Œä¼‘çœ æ–‡ä»¶',
                'solution': 'é«˜çº§ç³»ç»Ÿè®¾ç½® > æ€§èƒ½ > è™šæ‹Ÿå†…å­˜ > è‡ªå®šä¹‰å¤§å°æˆ–ç¦ç”¨ä¼‘çœ ',
            },
            'recycle_bin': {
                'paths': [
                    r'C:\\$Recycle.Bin',
                ],
                'name': 'å›æ”¶ç«™',
                'description': 'å·²åˆ é™¤ä½†æœªæ¸…ç©ºçš„æ–‡ä»¶',
                'solution': 'å³é”®å›æ”¶ç«™ > æ¸…ç©ºå›æ”¶ç«™',
            },
            'logs': {
                'paths': [
                    r'C:\\Windows\\Logs',
                    r'C:\\ProgramData\\.*\\logs',
                ],
                'name': 'æ—¥å¿—æ–‡ä»¶',
                'description': 'ç³»ç»Ÿå’Œåº”ç”¨ç¨‹åºæ—¥å¿—',
                'solution': 'äº‹ä»¶æŸ¥çœ‹å™¨ > å³é”®æ—¥å¿— > æ¸…é™¤æ—¥å¿—',
            },
        }

        import re
        for dir_info in growing_dirs:
            for culprit_type, pattern_info in patterns.items():
                for path_pattern in pattern_info['paths']:
                    if re.match(path_pattern, dir_info['path'], re.IGNORECASE):
                        culprits.append({
                            'type': culprit_type,
                            'name': pattern_info['name'],
                            'path': dir_info['path'],
                            'growth_mb': dir_info['growth_mb'],
                            'description': pattern_info['description'],
                            'solution': pattern_info['solution'],
                        })
                        break

        return culprits

    def _generate_recommendations(self, culprits: List[Dict]) -> List[str]:
        """ç”Ÿæˆæ¸…ç†å»ºè®®"""
        recommendations = []

        for culprit in culprits:
            recommendations.append(
                f"âš ï¸ {culprit['name']} (+{culprit['growth_mb']:.1f} MB): {culprit['solution']}"
            )

        return recommendations
```

**Treemapå¯è§†åŒ–ï¼ˆç±»ä¼¼WizTreeï¼‰**:
```python
import squarify  # pip install squarify
import matplotlib.pyplot as plt

class TreemapGenerator:
    """æ ‘å½¢å›¾ç”Ÿæˆå™¨ - å¯è§†åŒ–ç£ç›˜ç©ºé—´å ç”¨"""

    def generate_treemap(self, file_tree: Dict, max_depth: int = 3):
        """
        ç”ŸæˆTreemap

        file_tree æ ¼å¼:
        {
            'C:\\': {
                'size': 500GB,
                'children': {
                    'Windows': {'size': 50GB, 'children': {...}},
                    'Users': {'size': 200GB, 'children': {...}},
                    ...
                }
            }
        }
        """
        # æ‰å¹³åŒ–æ–‡ä»¶æ ‘ï¼ˆé™åˆ¶æ·±åº¦ï¼‰
        flat_data = self._flatten_tree(file_tree, max_depth)

        # å‡†å¤‡æ•°æ®
        sizes = [item['size'] for item in flat_data]
        labels = [
            f"{item['name']}\n{self._format_size(item['size'])}"
            for item in flat_data
        ]
        colors = self._generate_colors(len(flat_data))

        # ç»˜åˆ¶Treemap
        fig, ax = plt.subplots(figsize=(16, 9))
        squarify.plot(
            sizes=sizes,
            label=labels,
            color=colors,
            alpha=0.8,
            text_kwargs={'fontsize': 10, 'weight': 'bold'},
            ax=ax
        )
        ax.axis('off')
        plt.title('ç£ç›˜ç©ºé—´å ç”¨æ ‘å½¢å›¾', fontsize=20, weight='bold')

        return fig

    def _format_size(self, bytes_size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_size < 1024.0:
                return f"{bytes_size:.1f} {unit}"
            bytes_size /= 1024.0
        return f"{bytes_size:.1f} PB"
```

#### 3.2.3 å…³è”åˆ†æå™¨ (RelationAnalyzer)

**æ–‡ä»¶æ‹‰æ™®æ‹‰æ–¯å‘é‡æ˜ å°„ï¼ˆä¸HIDRSæ ¸å¿ƒæŠ€æœ¯ç»“åˆï¼‰**:
```python
from hidrs.laplacian_analyzer import LaplacianAnalyzer
from hidrs.holographic_mapping import HolisticMapping

class FileRelationAnalyzer:
    """æ–‡ä»¶å…³è”æ€§åˆ†æ - ç»“åˆHIDRSæ‹‰æ™®æ‹‰æ–¯è°±åˆ†æ"""

    def __init__(self):
        self.laplacian = LaplacianAnalyzer()
        self.holographic = HolisticMapping()

    def analyze_file_relations(self, file_list: List[str]) -> Dict:
        """åˆ†ææ–‡ä»¶ä¹‹é—´çš„éšè—å…³è”"""

        # ä¸ºæ¯ä¸ªæ–‡ä»¶è®¡ç®—ç‰¹å¾å‘é‡
        file_vectors = {}
        for file_path in file_list:
            # æå–æ–‡ä»¶å…ƒæ•°æ®ç‰¹å¾
            features = self._extract_file_features(file_path)

            # è®¡ç®—æ‹‰æ™®æ‹‰æ–¯å‘é‡ï¼ˆåŸºäºæ–‡ä»¶å±æ€§çš„å›¾ç»“æ„ï¼‰
            laplacian_vector = self.laplacian.compute_file_vector(features)

            # å…¨æ¯æ˜ å°„
            holographic_vector = self.holographic.embed(features)

            file_vectors[file_path] = {
                'laplacian': laplacian_vector,
                'holographic': holographic_vector,
            }

        # è®¡ç®—æ–‡ä»¶ä¹‹é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = self._compute_similarity_matrix(file_vectors)

        # èšç±»åˆ†æ - å‘ç°ç›¸å…³æ–‡ä»¶ç»„
        clusters = self._cluster_files(similarity_matrix)

        return {
            'clusters': clusters,
            'similarity_matrix': similarity_matrix,
            'related_files': self._find_related_files(similarity_matrix, threshold=0.8),
        }

    def _extract_file_features(self, file_path: str) -> Dict:
        """æå–æ–‡ä»¶ç‰¹å¾"""
        import os
        import hashlib

        stat = os.stat(file_path)

        # æ–‡ä»¶å¤´éƒ¨å“ˆå¸Œï¼ˆç”¨äºå†…å®¹ç›¸ä¼¼åº¦ï¼‰
        with open(file_path, 'rb') as f:
            header = f.read(4096)
            header_hash = hashlib.md5(header).hexdigest()

        return {
            'size': stat.st_size,
            'created': stat.st_ctime,
            'modified': stat.st_mtime,
            'extension': os.path.splitext(file_path)[1].lower(),
            'header_hash': header_hash,
            'directory': os.path.dirname(file_path),
            'depth': file_path.count(os.sep),
        }
```

**é‡å¤æ–‡ä»¶æ£€æµ‹**:
```python
import hashlib
from collections import defaultdict

class DuplicateFinder:
    """é‡å¤æ–‡ä»¶æ£€æµ‹ - è¯†åˆ«æµªè´¹çš„ç£ç›˜ç©ºé—´"""

    def find_duplicates(self, file_list: List[str]) -> List[List[str]]:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶ï¼ˆåŸºäºå†…å®¹å“ˆå¸Œï¼‰"""

        # ç¬¬ä¸€éï¼šæŒ‰å¤§å°åˆ†ç»„ï¼ˆå¿«é€Ÿè¿‡æ»¤ï¼‰
        size_groups = defaultdict(list)
        for file_path in file_list:
            try:
                size = os.path.getsize(file_path)
                size_groups[size].append(file_path)
            except:
                continue

        # ç¬¬äºŒéï¼šå¯¹ç›¸åŒå¤§å°çš„æ–‡ä»¶è®¡ç®—å“ˆå¸Œ
        hash_groups = defaultdict(list)
        for size, files in size_groups.items():
            if len(files) < 2:  # åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡
                continue

            for file_path in files:
                try:
                    file_hash = self._compute_file_hash(file_path)
                    hash_groups[file_hash].append(file_path)
                except:
                    continue

        # è¿”å›é‡å¤æ–‡ä»¶ç»„
        duplicates = [
            files for files in hash_groups.values() if len(files) > 1
        ]

        # è®¡ç®—æµªè´¹çš„ç©ºé—´
        wasted_space = 0
        for dup_group in duplicates:
            file_size = os.path.getsize(dup_group[0])
            wasted_space += file_size * (len(dup_group) - 1)

        return {
            'duplicate_groups': duplicates,
            'total_groups': len(duplicates),
            'total_duplicates': sum(len(g) - 1 for g in duplicates),
            'wasted_space_mb': wasted_space / (1024 * 1024),
        }

    def _compute_file_hash(self, file_path: str, algorithm='md5') -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆæ”¯æŒå¤§æ–‡ä»¶ï¼‰"""
        hash_func = hashlib.new(algorithm)

        with open(file_path, 'rb') as f:
            # åˆ†å—è¯»å–ï¼Œé¿å…å†…å­˜æº¢å‡º
            for chunk in iter(lambda: f.read(8192), b''):
                hash_func.update(chunk)

        return hash_func.hexdigest()
```

#### 3.2.4 æ—¶é—´çº¿è¿½è¸ª (TimelineTracker)

**æ–‡ä»¶å¢é•¿è¶‹åŠ¿åˆ†æ**:
```python
import json
import time
from collections import defaultdict

class TimelineTracker:
    """æ—¶é—´çº¿è¿½è¸ªå™¨ - ç›‘æ§æ–‡ä»¶å¢é•¿è¶‹åŠ¿"""

    def __init__(self, snapshot_dir: str = './snapshots'):
        self.snapshot_dir = snapshot_dir
        os.makedirs(snapshot_dir, exist_ok=True)

    def create_snapshot(self, name: str = None):
        """åˆ›å»ºç£ç›˜çŠ¶æ€å¿«ç…§"""
        if name is None:
            name = f"snapshot_{int(time.time())}"

        snapshot = {
            'name': name,
            'timestamp': time.time(),
            'datetime': time.strftime('%Y-%m-%d %H:%M:%S'),
            'file_count': 0,
            'total_size': 0,
            'files_by_dir': defaultdict(lambda: {'count': 0, 'size': 0}),
            'files_by_ext': defaultdict(lambda: {'count': 0, 'size': 0}),
            'top_files': [],  # å‰100ä¸ªæœ€å¤§æ–‡ä»¶
        }

        # éå†æ‰€æœ‰æ–‡ä»¶
        all_files = []
        for root, dirs, files in os.walk('C:\\'):
            for file in files:
                file_path = os.path.join(root, file)
                try:
                    size = os.path.getsize(file_path)
                    ext = os.path.splitext(file)[1].lower()

                    snapshot['file_count'] += 1
                    snapshot['total_size'] += size

                    snapshot['files_by_dir'][root]['count'] += 1
                    snapshot['files_by_dir'][root]['size'] += size

                    snapshot['files_by_ext'][ext]['count'] += 1
                    snapshot['files_by_ext'][ext]['size'] += size

                    all_files.append({'path': file_path, 'size': size})
                except:
                    continue

        # ä¿å­˜å‰100ä¸ªæœ€å¤§æ–‡ä»¶
        all_files.sort(key=lambda x: x['size'], reverse=True)
        snapshot['top_files'] = all_files[:100]

        # ä¿å­˜å¿«ç…§
        snapshot_path = os.path.join(self.snapshot_dir, f"{name}.json")
        with open(snapshot_path, 'w', encoding='utf-8') as f:
            json.dump(snapshot, f, indent=2, ensure_ascii=False)

        return snapshot

    def compare_snapshots(self, snapshot1_name: str, snapshot2_name: str) -> Dict:
        """å¯¹æ¯”ä¸¤ä¸ªå¿«ç…§ï¼Œæ‰¾å‡ºå·®å¼‚"""

        snap1 = self._load_snapshot(snapshot1_name)
        snap2 = self._load_snapshot(snapshot2_name)

        delta = {
            'time_elapsed_hours': (snap2['timestamp'] - snap1['timestamp']) / 3600,
            'file_count_change': snap2['file_count'] - snap1['file_count'],
            'size_change_mb': (snap2['total_size'] - snap1['total_size']) / (1024 * 1024),
            'growth_rate_mb_per_hour': 0,
        }

        delta['growth_rate_mb_per_hour'] = (
            delta['size_change_mb'] / delta['time_elapsed_hours']
        )

        # æ‰¾å‡ºå¢é•¿æœ€å¿«çš„ç›®å½•
        dir_changes = []
        for dir_path in snap2['files_by_dir']:
            size1 = snap1['files_by_dir'].get(dir_path, {}).get('size', 0)
            size2 = snap2['files_by_dir'][dir_path]['size']
            growth = size2 - size1

            if growth > 10 * 1024 * 1024:  # å¢é•¿è¶…è¿‡10MB
                dir_changes.append({
                    'path': dir_path,
                    'growth_mb': growth / (1024 * 1024),
                    'size_before_mb': size1 / (1024 * 1024),
                    'size_after_mb': size2 / (1024 * 1024),
                })

        dir_changes.sort(key=lambda x: x['growth_mb'], reverse=True)

        # æ‰¾å‡ºæ–°å¢çš„å¤§æ–‡ä»¶
        snap1_top_paths = {f['path'] for f in snap1['top_files']}
        new_large_files = [
            f for f in snap2['top_files'] if f['path'] not in snap1_top_paths
        ]

        return {
            'summary': delta,
            'dir_changes': dir_changes[:20],
            'new_large_files': new_large_files[:20],
        }

    def _load_snapshot(self, name: str) -> Dict:
        """åŠ è½½å¿«ç…§"""
        snapshot_path = os.path.join(self.snapshot_dir, f"{name}.json")
        with open(snapshot_path, 'r', encoding='utf-8') as f:
            return json.load(f)
```

---

## 4. æ’ä»¶å®ç°

### 4.1 æ’ä»¶ä¸»ç±»

```python
from plugins.base import HIDRSPlugin
from typing import Dict, List, Any

class LocalFileSearchPlugin(HIDRSPlugin):
    """HIDRSæœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶"""

    PLUGIN_NAME = "local_file_search"
    PLUGIN_VERSION = "1.0.0"
    REQUIRED_PERMISSIONS = ['filesystem.read', 'filesystem.index', 'system.process']

    def __init__(self, config: Dict[str, Any]):
        super().__init__(config)

        # ç´¢å¼•å¼•æ“
        if os.name == 'nt':  # Windows
            from .mft_reader import MFTReader
            self.indexer = MFTReader(config.get('drive', 'C'))
        else:  # Linux/Mac
            from .file_watcher import FileSystemWatcher
            self.indexer = FileSystemWatcher(config.get('root', '/'))

        # ç©ºé—´åˆ†æå™¨
        self.space_analyzer = SpaceAnalyzer(self.indexer)
        self.leak_detector = SpaceLeakDetector(self.indexer)

        # å…³è”åˆ†æå™¨
        self.relation_analyzer = FileRelationAnalyzer()
        self.duplicate_finder = DuplicateFinder()

        # æ—¶é—´çº¿è¿½è¸ª
        self.timeline = TimelineTracker(config.get('snapshot_dir', './snapshots'))

    def initialize(self) -> bool:
        """æ’ä»¶åˆå§‹åŒ–"""
        try:
            logger.info(f"[{self.PLUGIN_NAME}] å¼€å§‹å»ºç«‹æ–‡ä»¶ç´¢å¼•...")

            # å»ºç«‹åˆå§‹ç´¢å¼•
            self.indexer.build_index()

            # å¯åŠ¨å®æ—¶ç›‘æ§
            self.indexer.start_monitoring()

            logger.info(f"[{self.PLUGIN_NAME}] ç´¢å¼•å®Œæˆï¼Œå…± {self.indexer.get_file_count()} ä¸ªæ–‡ä»¶")
            return True
        except Exception as e:
            logger.error(f"[{self.PLUGIN_NAME}] åˆå§‹åŒ–å¤±è´¥: {e}")
            return False

    def execute(self, query: str, **kwargs) -> List[Dict[str, Any]]:
        """æ‰§è¡ŒæŸ¥è¯¢"""
        search_type = kwargs.get('type', 'name')  # name, content, signature, space

        if search_type == 'name':
            return self._search_by_name(query, **kwargs)
        elif search_type == 'content':
            return self._search_by_content(query, **kwargs)
        elif search_type == 'space_leak':
            return self._diagnose_space_leak(**kwargs)
        elif search_type == 'duplicates':
            return self._find_duplicates(**kwargs)
        elif search_type == 'timeline':
            return self._timeline_analysis(**kwargs)
        else:
            raise ValueError(f"æœªçŸ¥çš„æœç´¢ç±»å‹: {search_type}")

    def _search_by_name(self, pattern: str, **kwargs) -> List[Dict]:
        """æŒ‰æ–‡ä»¶åæœç´¢ï¼ˆæ”¯æŒæ­£åˆ™ï¼‰"""
        import re
        regex = re.compile(pattern, re.IGNORECASE)

        results = []
        for file_info in self.indexer.get_all_files():
            if regex.search(file_info['name']):
                results.append(file_info)

        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
        results.sort(key=lambda x: x.get('modified', 0), reverse=True)

        return results[:kwargs.get('limit', 1000)]

    def _diagnose_space_leak(self, **kwargs) -> Dict:
        """è¯Šæ–­ç£ç›˜ç©ºé—´"çƒ­èƒ€å†·ç¼©"é—®é¢˜"""

        # å¦‚æœæ²¡æœ‰åŸºçº¿ï¼Œå…ˆåˆ›å»º
        if not self.leak_detector.baseline:
            logger.warning("æœªæ‰¾åˆ°åŸºçº¿å¿«ç…§ï¼Œæ­£åœ¨åˆ›å»º...")
            self.leak_detector.create_baseline_snapshot()
            return {
                'status': 'baseline_created',
                'message': 'åŸºçº¿å¿«ç…§å·²åˆ›å»ºï¼Œè¯·åœ¨æ¸…ç†ç£ç›˜åå†æ¬¡è¿è¡Œè¯Šæ–­',
            }

        # æ‰§è¡Œæ³„æ¼æ£€æµ‹
        interval = kwargs.get('interval_minutes', 60)
        report = self.leak_detector.detect_space_leak(interval)

        return report

    def _find_duplicates(self, **kwargs) -> Dict:
        """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
        scan_path = kwargs.get('path', 'C:\\')
        min_size = kwargs.get('min_size_mb', 1) * 1024 * 1024

        # è·å–æ‰€æœ‰æ–‡ä»¶ï¼ˆè¿‡æ»¤å°æ–‡ä»¶ï¼‰
        all_files = [
            f['path'] for f in self.indexer.get_all_files()
            if f['size'] >= min_size and f['path'].startswith(scan_path)
        ]

        logger.info(f"æ­£åœ¨æ‰«æ {len(all_files)} ä¸ªæ–‡ä»¶...")

        # æŸ¥æ‰¾é‡å¤
        duplicates = self.duplicate_finder.find_duplicates(all_files)

        return duplicates

    def _timeline_analysis(self, **kwargs) -> Dict:
        """æ—¶é—´çº¿åˆ†æ"""
        action = kwargs.get('action', 'snapshot')

        if action == 'snapshot':
            # åˆ›å»ºå¿«ç…§
            name = kwargs.get('name')
            snapshot = self.timeline.create_snapshot(name)
            return {
                'status': 'snapshot_created',
                'snapshot': snapshot,
            }

        elif action == 'compare':
            # å¯¹æ¯”å¿«ç…§
            snap1 = kwargs.get('snapshot1')
            snap2 = kwargs.get('snapshot2')
            comparison = self.timeline.compare_snapshots(snap1, snap2)
            return {
                'status': 'comparison_completed',
                'comparison': comparison,
            }

        else:
            raise ValueError(f"æœªçŸ¥çš„æ—¶é—´çº¿æ“ä½œ: {action}")

    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®"""
        required_keys = ['drive' if os.name == 'nt' else 'root']

        for key in required_keys:
            if key not in self.config:
                logger.error(f"ç¼ºå°‘å¿…éœ€é…ç½®é¡¹: {key}")
                return False

        return True
```

### 4.2 é…ç½®æ–‡ä»¶

```yaml
# plugins/local_file_search/config.yaml

name: LocalFileSearchPlugin
enabled: true
version: 1.0.0

# Windowsé…ç½®
drive: C  # æ‰«æçš„é©±åŠ¨å™¨

# Linux/Macé…ç½®
root: /home  # æ‰«æçš„æ ¹ç›®å½•

# ç´¢å¼•é…ç½®
index:
  auto_update: true  # è‡ªåŠ¨å¢é‡æ›´æ–°
  exclude_patterns:  # æ’é™¤çš„è·¯å¾„æ¨¡å¼
    - "*/node_modules/*"
    - "*/.git/*"
    - "*/venv/*"
    - "*/temp/*"
  max_file_size_mb: 1024  # æœ€å¤§ç´¢å¼•æ–‡ä»¶å¤§å°ï¼ˆè¶…è¿‡åˆ™è·³è¿‡å†…å®¹ç´¢å¼•ï¼‰

# ç©ºé—´åˆ†æé…ç½®
space_analysis:
  baseline_auto_create: false  # é¦–æ¬¡è¿è¡Œè‡ªåŠ¨åˆ›å»ºåŸºçº¿
  monitoring_interval_minutes: 60  # ç›‘æ§é—´éš”
  growth_threshold_mb: 100  # å¢é•¿é˜ˆå€¼ï¼ˆè¶…è¿‡åˆ™å‘Šè­¦ï¼‰

# é‡å¤æ–‡ä»¶æ£€æµ‹é…ç½®
duplicate_detection:
  min_file_size_mb: 1  # æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå°äºåˆ™å¿½ç•¥ï¼‰
  hash_algorithm: md5  # å“ˆå¸Œç®—æ³• (md5, sha1, sha256)

# å¿«ç…§é…ç½®
snapshot:
  dir: ./snapshots  # å¿«ç…§ä¿å­˜ç›®å½•
  retention_days: 30  # å¿«ç…§ä¿ç•™å¤©æ•°

# æƒé™
permissions:
  - filesystem.read
  - filesystem.index
  - system.process
```

---

## 5. å‰ç«¯é›†æˆ

### 5.1 æœ¬åœ°æ–‡ä»¶æœç´¢ç•Œé¢

```html
<!-- plugins/local_file_search/templates/search.html -->

<div class="local-file-search-panel">
    <h3>ğŸ—‚ï¸ æœ¬åœ°æ–‡ä»¶æœç´¢</h3>

    <!-- æœç´¢ç±»å‹é€‰æ‹© -->
    <ul class="nav nav-tabs" id="searchTypeTabs">
        <li class="nav-item">
            <a class="nav-link active" data-tab="name-search">æ–‡ä»¶åæœç´¢</a>
        </li>
        <li class="nav-item">
            <a class="nav-link" data-tab="space-leak">ç©ºé—´æ³„æ¼è¯Šæ–­</a>
        </li>
        <li class="nav-item">
            <a class="nav-link" data-tab="duplicates">é‡å¤æ–‡ä»¶æ£€æµ‹</a>
        </li>
        <li class="nav-item">
            <a class="nav-link" data-tab="timeline">æ—¶é—´çº¿åˆ†æ</a>
        </li>
    </ul>

    <!-- æ–‡ä»¶åæœç´¢ -->
    <div id="name-search" class="search-tab-content active">
        <div class="input-group mb-3">
            <input type="text" class="form-control" id="file-name-input"
                   placeholder="è¾“å…¥æ–‡ä»¶åæˆ–æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚ï¼š.*\.log$ï¼‰">
            <button class="btn btn-primary" id="search-file-name-btn">æœç´¢</button>
        </div>

        <div id="file-search-results"></div>
    </div>

    <!-- ç©ºé—´æ³„æ¼è¯Šæ–­ -->
    <div id="space-leak" class="search-tab-content">
        <div class="alert alert-info">
            <strong>ä½¿ç”¨è¯´æ˜:</strong>
            <ol>
                <li>æ¸…ç†ç£ç›˜å‰ï¼Œç‚¹å‡»"åˆ›å»ºåŸºçº¿å¿«ç…§"</li>
                <li>æ¸…ç†ç£ç›˜åï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´ï¼ˆå»ºè®®1-2å°æ—¶ï¼‰</li>
                <li>ç‚¹å‡»"æ‰§è¡Œè¯Šæ–­"æŸ¥çœ‹ç©ºé—´å›å¡«åŸå› </li>
            </ol>
        </div>

        <div class="btn-group mb-3">
            <button class="btn btn-success" id="create-baseline-btn">åˆ›å»ºåŸºçº¿å¿«ç…§</button>
            <button class="btn btn-danger" id="diagnose-leak-btn">æ‰§è¡Œè¯Šæ–­</button>
        </div>

        <div id="space-leak-report"></div>
    </div>

    <!-- é‡å¤æ–‡ä»¶æ£€æµ‹ -->
    <div id="duplicates" class="search-tab-content">
        <div class="input-group mb-3">
            <input type="text" class="form-control" id="duplicate-scan-path"
                   placeholder="æ‰«æè·¯å¾„ï¼ˆå¦‚ï¼šC:\Usersï¼‰" value="C:\">
            <input type="number" class="form-control" id="duplicate-min-size"
                   placeholder="æœ€å°æ–‡ä»¶å¤§å°(MB)" value="1">
            <button class="btn btn-primary" id="find-duplicates-btn">æŸ¥æ‰¾é‡å¤æ–‡ä»¶</button>
        </div>

        <div id="duplicate-results"></div>
    </div>

    <!-- æ—¶é—´çº¿åˆ†æ -->
    <div id="timeline" class="search-tab-content">
        <div class="btn-group mb-3">
            <button class="btn btn-success" id="create-snapshot-btn">åˆ›å»ºå¿«ç…§</button>
            <button class="btn btn-primary" id="compare-snapshots-btn">å¯¹æ¯”å¿«ç…§</button>
        </div>

        <div id="snapshot-list"></div>
        <div id="timeline-comparison"></div>
    </div>
</div>
```

### 5.2 ç©ºé—´æ³„æ¼è¯Šæ–­æŠ¥å‘Šæ¨¡æ¿

```javascript
// frontend/js/local_file_search_ui.js

function renderSpaceLeakReport(report) {
    const container = document.getElementById('space-leak-report');

    const html = `
        <div class="space-leak-report">
            <h4>ğŸ“Š è¯Šæ–­æŠ¥å‘Š</h4>

            <!-- æ¦‚è¦ -->
            <div class="alert alert-warning">
                <h5>æ¦‚è¦</h5>
                <ul>
                    <li>æ—¶é—´é—´éš”: ${report.summary.time_elapsed.toFixed(1)} åˆ†é’Ÿ</li>
                    <li>ç©ºé—´å¢é•¿: ${report.summary.size_increase_mb.toFixed(1)} MB</li>
                    <li>æ–‡ä»¶å¢åŠ : ${report.summary.file_increase} ä¸ª</li>
                    <li>å¢é•¿é€Ÿåº¦: ${report.summary.growth_rate_mb_per_hour.toFixed(2)} MB/å°æ—¶</li>
                </ul>
            </div>

            <!-- å¿«é€Ÿå¢é•¿çš„ç›®å½• -->
            <h5>ğŸš€ å¿«é€Ÿå¢é•¿çš„ç›®å½•ï¼ˆTop 10ï¼‰</h5>
            <table class="table table-striped">
                <thead>
                    <tr>
                        <th>è·¯å¾„</th>
                        <th>å¢é•¿(MB)</th>
                        <th>å¢é•¿ç‡</th>
                        <th>æ“ä½œ</th>
                    </tr>
                </thead>
                <tbody>
                    ${report.fast_growing_dirs.slice(0, 10).map(dir => `
                        <tr>
                            <td><code>${dir.path}</code></td>
                            <td>${dir.growth_mb.toFixed(1)}</td>
                            <td>${dir.growth_rate.toFixed(1)}%</td>
                            <td>
                                <button class="btn btn-sm btn-primary"
                                        onclick="openFolder('${dir.path}')">
                                    æ‰“å¼€
                                </button>
                            </td>
                        </tr>
                    `).join('')}
                </tbody>
            </table>

            <!-- è¯†åˆ«çš„"ç½ªé­ç¥¸é¦–" -->
            <h5>âš ï¸ è¯†åˆ«çš„"ç½ªé­ç¥¸é¦–"</h5>
            ${report.culprits.map(culprit => `
                <div class="alert alert-danger">
                    <h6>${culprit.name} (+${culprit.growth_mb.toFixed(1)} MB)</h6>
                    <p>${culprit.description}</p>
                    <p><strong>è·¯å¾„:</strong> <code>${culprit.path}</code></p>
                    <p><strong>è§£å†³æ–¹æ¡ˆ:</strong> ${culprit.solution}</p>
                </div>
            `).join('')}

            <!-- æ¸…ç†å»ºè®® -->
            <h5>ğŸ’¡ æ¸…ç†å»ºè®®</h5>
            <ul>
                ${report.recommendations.map(rec => `<li>${rec}</li>`).join('')}
            </ul>
        </div>
    `;

    container.innerHTML = html;
}
```

### 5.3 Treemapå¯è§†åŒ–ï¼ˆEChartsï¼‰

```javascript
// frontend/js/treemap_visualizer.js

function renderTreemap(fileTree) {
    const chartDom = document.getElementById('treemap-chart');
    const myChart = echarts.init(chartDom);

    // è½¬æ¢æ•°æ®æ ¼å¼
    const data = convertToEChartsFormat(fileTree);

    const option = {
        title: {
            text: 'ç£ç›˜ç©ºé—´å ç”¨æ ‘å½¢å›¾',
            left: 'center',
        },
        tooltip: {
            formatter: function (info) {
                return [
                    `<div class="tooltip-title">${info.name}</div>`,
                    `å¤§å°: ${formatSize(info.value)}`,
                    `å æ¯”: ${info.percent}%`,
                ].join('');
            }
        },
        series: [
            {
                type: 'treemap',
                data: data,
                label: {
                    show: true,
                    formatter: '{b}\n{c}',
                },
                itemStyle: {
                    borderColor: '#fff',
                    borderWidth: 2,
                },
                levels: [
                    {
                        itemStyle: {
                            borderColor: '#777',
                            borderWidth: 3,
                            gapWidth: 3,
                        }
                    },
                    {
                        colorSaturation: [0.35, 0.5],
                        itemStyle: {
                            gapWidth: 1,
                            borderColorSaturation: 0.6,
                        }
                    }
                ],
            }
        ]
    };

    myChart.setOption(option);
}

function convertToEChartsFormat(fileTree) {
    const result = [];

    for (const [name, node] of Object.entries(fileTree.children)) {
        const item = {
            name: name,
            value: node.size,
        };

        if (node.children && Object.keys(node.children).length > 0) {
            item.children = convertToEChartsFormat(node);
        }

        result.push(item);
    }

    return result;
}
```

---

## 6. APIç«¯ç‚¹

### 6.1 åç«¯è·¯ç”±ï¼ˆFlaskï¼‰

```python
# backend/crawler_server.py

@app.route('/api/local-file/search', methods=['POST'])
def local_file_search():
    """æœ¬åœ°æ–‡ä»¶æœç´¢"""
    data = request.json
    query = data.get('query', '')
    search_type = data.get('type', 'name')

    plugin = plugin_manager.get_plugin('local_file_search')
    if not plugin:
        return jsonify({'error': 'æœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶æœªå¯ç”¨'}), 400

    try:
        results = plugin.execute(query, type=search_type, **data)
        return jsonify({'success': True, 'data': results})
    except Exception as e:
        logger.error(f"æœ¬åœ°æ–‡ä»¶æœç´¢å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/local-file/diagnose-space-leak', methods=['POST'])
def diagnose_space_leak():
    """è¯Šæ–­ç£ç›˜ç©ºé—´æ³„æ¼"""
    data = request.json

    plugin = plugin_manager.get_plugin('local_file_search')
    if not plugin:
        return jsonify({'error': 'æœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶æœªå¯ç”¨'}), 400

    try:
        report = plugin.execute('', type='space_leak', **data)
        return jsonify({'success': True, 'data': report})
    except Exception as e:
        logger.error(f"ç©ºé—´æ³„æ¼è¯Šæ–­å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/local-file/create-baseline', methods=['POST'])
def create_baseline():
    """åˆ›å»ºåŸºçº¿å¿«ç…§"""
    plugin = plugin_manager.get_plugin('local_file_search')
    if not plugin:
        return jsonify({'error': 'æœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶æœªå¯ç”¨'}), 400

    try:
        plugin.leak_detector.create_baseline_snapshot()
        return jsonify({'success': True, 'message': 'åŸºçº¿å¿«ç…§å·²åˆ›å»º'})
    except Exception as e:
        logger.error(f"åˆ›å»ºåŸºçº¿å¿«ç…§å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/local-file/find-duplicates', methods=['POST'])
def find_duplicates():
    """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
    data = request.json

    plugin = plugin_manager.get_plugin('local_file_search')
    if not plugin:
        return jsonify({'error': 'æœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶æœªå¯ç”¨'}), 400

    try:
        duplicates = plugin.execute('', type='duplicates', **data)
        return jsonify({'success': True, 'data': duplicates})
    except Exception as e:
        logger.error(f"é‡å¤æ–‡ä»¶æ£€æµ‹å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/local-file/timeline', methods=['POST'])
def timeline_operation():
    """æ—¶é—´çº¿æ“ä½œï¼ˆå¿«ç…§ã€å¯¹æ¯”ï¼‰"""
    data = request.json

    plugin = plugin_manager.get_plugin('local_file_search')
    if not plugin:
        return jsonify({'error': 'æœ¬åœ°æ–‡ä»¶æœç´¢æ’ä»¶æœªå¯ç”¨'}), 400

    try:
        result = plugin.execute('', type='timeline', **data)
        return jsonify({'success': True, 'data': result})
    except Exception as e:
        logger.error(f"æ—¶é—´çº¿æ“ä½œå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500
```

---

## 7. å®ç°è·¯çº¿å›¾

### Phase 1: æ ¸å¿ƒç´¢å¼•å¼•æ“ï¼ˆ2-3å‘¨ï¼‰

**Week 1-2: ç´¢å¼•å¼•æ“å¼€å‘**
- [ ] Windows MFT Readerå®ç°
- [ ] Linux/Macæ–‡ä»¶ç³»ç»Ÿç›‘æ§ï¼ˆwatchdogï¼‰
- [ ] å¢é‡ç´¢å¼•æ›´æ–°æœºåˆ¶
- [ ] ç´¢å¼•æ•°æ®åº“è®¾è®¡ï¼ˆSQLiteï¼‰

**Week 3: åŸºç¡€æœç´¢åŠŸèƒ½**
- [ ] æ–‡ä»¶åæ­£åˆ™æœç´¢
- [ ] åŸºç¡€ç­›é€‰ï¼ˆå¤§å°ã€æ—¥æœŸã€ç±»å‹ï¼‰
- [ ] APIç«¯ç‚¹å®ç°

### Phase 2: ç©ºé—´åˆ†æä¸è¯Šæ–­ï¼ˆ2-3å‘¨ï¼‰

**Week 4: ç©ºé—´åˆ†æå™¨**
- [ ] ç£ç›˜ç©ºé—´ç»Ÿè®¡
- [ ] æ–‡ä»¶ç±»å‹åˆ†ç»„
- [ ] ç›®å½•æ ‘å¤§å°è®¡ç®—

**Week 5: ç©ºé—´æ³„æ¼æ£€æµ‹**
- [ ] åŸºçº¿å¿«ç…§ç³»ç»Ÿ
- [ ] å¢é‡å¯¹æ¯”ç®—æ³•
- [ ] "ç½ªé­ç¥¸é¦–"è¯†åˆ«è§„åˆ™åº“

**Week 6: Treemapå¯è§†åŒ–**
- [ ] åç«¯æ•°æ®æ ¼å¼åŒ–
- [ ] ECharts Treemapé›†æˆ
- [ ] äº¤äº’å¼é’»å–åŠŸèƒ½

### Phase 3: é«˜çº§åˆ†æåŠŸèƒ½ï¼ˆ2-3å‘¨ï¼‰

**Week 7: é‡å¤æ–‡ä»¶æ£€æµ‹**
- [ ] å“ˆå¸Œè®¡ç®—ä¼˜åŒ–ï¼ˆåˆ†å—è¯»å–ï¼‰
- [ ] ç›¸ä¼¼åº¦åˆ†æï¼ˆfuzzy hashï¼‰
- [ ] æ‰¹é‡åˆ é™¤æ¥å£

**Week 8: æ–‡ä»¶å…³è”åˆ†æ**
- [ ] æ–‡ä»¶ç‰¹å¾æå–
- [ ] æ‹‰æ™®æ‹‰æ–¯å‘é‡é›†æˆ
- [ ] å…³è”å›¾å¯è§†åŒ–

**Week 9: æ—¶é—´çº¿è¿½è¸ª**
- [ ] å¿«ç…§ç³»ç»Ÿå®ç°
- [ ] å¿«ç…§å¯¹æ¯”ç®—æ³•
- [ ] è¶‹åŠ¿å›¾è¡¨ï¼ˆChart.jsï¼‰

### Phase 4: å‰ç«¯UIä¸ä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰

**Week 10: å‰ç«¯å¼€å‘**
- [ ] æœç´¢ç•Œé¢å®ç°
- [ ] è¯Šæ–­æŠ¥å‘Šç•Œé¢
- [ ] å¯è§†åŒ–å›¾è¡¨é›†æˆ

**Week 11: æµ‹è¯•ä¸ä¼˜åŒ–**
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆå¤§æ–‡ä»¶å¤„ç†ï¼‰
- [ ] é”™è¯¯å¤„ç†å®Œå–„
- [ ] ç”¨æˆ·æ–‡æ¡£ç¼–å†™

---

## 8. æŠ€æœ¯éš¾ç‚¹ä¸è§£å†³æ–¹æ¡ˆ

### 8.1 MFTè¯»å–æƒé™é—®é¢˜

**é—®é¢˜**: ç›´æ¥è¯»å–MFTéœ€è¦ç®¡ç†å‘˜æƒé™

**è§£å†³æ–¹æ¡ˆ**:
1. æç¤ºç”¨æˆ·ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ
2. é™çº§åˆ°å¸¸è§„æ–‡ä»¶éå†ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰
3. ä½¿ç”¨Windows Search APIä½œä¸ºå¤‡é€‰

### 8.2 å¤§æ–‡ä»¶å“ˆå¸Œè®¡ç®—æ€§èƒ½

**é—®é¢˜**: è®¡ç®—TBçº§ç¡¬ç›˜çš„æ–‡ä»¶å“ˆå¸Œè€—æ—¶è¿‡é•¿

**è§£å†³æ–¹æ¡ˆ**:
1. åˆ†å—è¯»å–ï¼ˆ8KB chunksï¼‰
2. å¤šè¿›ç¨‹å¹¶è¡Œè®¡ç®—ï¼ˆProcessPoolExecutorï¼‰
3. æ™ºèƒ½é‡‡æ ·ï¼ˆä»…å“ˆå¸Œæ–‡ä»¶å¤´+å°¾ï¼Œå¿«é€Ÿåˆç­›ï¼‰
4. å¢é‡æ›´æ–°ï¼ˆä»…è®¡ç®—æ–°æ–‡ä»¶/ä¿®æ”¹æ–‡ä»¶ï¼‰

### 8.3 è·¨å¹³å°å…¼å®¹æ€§

**é—®é¢˜**: Windows MFTè¯»å–æ— æ³•ç”¨äºLinux/Mac

**è§£å†³æ–¹æ¡ˆ**:
- Windows: MFTç›´æ¥è¯»å–ï¼ˆæœ€å¿«ï¼‰
- Linux: ä½¿ç”¨inotify + åˆå§‹å…¨ç›˜æ‰«æ
- Mac: ä½¿ç”¨FSEvents + åˆå§‹å…¨ç›˜æ‰«æ

---

## 9. ä¸ç°æœ‰å·¥å…·çš„å·®å¼‚åŒ–ä¼˜åŠ¿

| åŠŸèƒ½ | Everything | WizTree | TreeSize | **HIDRSæœ¬åœ°æœç´¢æ’ä»¶** |
|------|-----------|---------|----------|----------------------|
| æ–‡ä»¶åæœç´¢ | âœ… æå¿« | âœ… å¿« | âœ… ä¸­ç­‰ | âœ… å¿«ï¼ˆMFTï¼‰ |
| å†…å®¹æœç´¢ | âš ï¸ v1.5+ | âŒ | âŒ | âœ… å…¨æ–‡ç´¢å¼• |
| ç©ºé—´å¯è§†åŒ– | âŒ | âœ… Treemap | âœ… å¤šç§è§†å›¾ | âœ… Treemap + ç»Ÿè®¡ |
| é‡å¤æ–‡ä»¶æ£€æµ‹ | âŒ | âŒ | âœ… | âœ… å“ˆå¸Œ+ç›¸ä¼¼åº¦ |
| **ç©ºé—´æ³„æ¼è¯Šæ–­** | âŒ | âŒ | âŒ | âœ… **ç‹¬æœ‰** |
| **æ–‡ä»¶å…³è”åˆ†æ** | âŒ | âŒ | âŒ | âœ… **ç‹¬æœ‰ï¼ˆæ‹‰æ™®æ‹‰æ–¯ï¼‰** |
| **æ—¶é—´çº¿è¿½è¸ª** | âŒ | âŒ | âš ï¸ å¹´é¾„è§†å›¾ | âœ… **å®Œæ•´å¿«ç…§ç³»ç»Ÿ** |
| è·¨å¹³å° | âŒ Windows | âŒ Windows | âŒ Windows | âœ… Win/Linux/Mac |

### ç‹¬ç‰¹ä»·å€¼

1. **æ™ºèƒ½è¯Šæ–­**: ä¸ä»…æ˜¾ç¤ºç£ç›˜å ç”¨ï¼Œè¿˜èƒ½è¯Šæ–­"æ¸…ç†åç«‹åˆ»å›å¡«"çš„æ ¹æœ¬åŸå› 
2. **HIDRSç”Ÿæ€é›†æˆ**: ä¸æ‹‰æ™®æ‹‰æ–¯åˆ†æã€å…¨æ¯æ˜ å°„ã€éƒ½å¸‚ä¼ è¯´æ£€æµ‹ç­‰åŠŸèƒ½æ·±åº¦ç»“åˆ
3. **ç ”ç©¶å¯¼å‘**: å‘ç°æ–‡ä»¶ä¹‹é—´çš„éšè—å…³è”ï¼ˆå¦‚ï¼šå“ªäº›ä¸´æ—¶æ–‡ä»¶å±äºåŒä¸€åº”ç”¨ï¼‰
4. **å¼€æºå…è´¹**: æ‰€æœ‰åŠŸèƒ½å®Œå…¨å¼€æºï¼Œæ— å•†ä¸šé™åˆ¶

---

## 10. å®‰å…¨ä¸éšç§

### 10.1 æƒé™æ§åˆ¶

- **åªè¯»è®¿é—®**: æ’ä»¶ä»…è¯»å–æ–‡ä»¶å…ƒæ•°æ®ï¼Œä¸ä¿®æ”¹ä»»ä½•æ–‡ä»¶
- **ç”¨æˆ·ç¡®è®¤**: åˆ é™¤æ“ä½œï¼ˆå¦‚é‡å¤æ–‡ä»¶ï¼‰éœ€ç”¨æˆ·æ˜ç¡®ç¡®è®¤
- **æœ¬åœ°å¤„ç†**: æ‰€æœ‰æ•°æ®å¤„ç†å‡åœ¨æœ¬åœ°ï¼Œä¸ä¸Šä¼ åˆ°æœåŠ¡å™¨

### 10.2 æ•æ„Ÿæ•°æ®ä¿æŠ¤

- **æ’é™¤æ¨¡å¼**: é»˜è®¤æ’é™¤ç³»ç»Ÿæ–‡ä»¶ã€å¯†é’¥æ–‡ä»¶ï¼ˆå¦‚`.ssh/`, `.gnupg/`ï¼‰
- **å“ˆå¸Œä¸å¯é€†**: æ–‡ä»¶å“ˆå¸Œä½¿ç”¨å•å‘ç®—æ³•ï¼Œæ— æ³•è¿˜åŸå†…å®¹
- **å¿«ç…§åŠ å¯†**: å¿«ç…§æ–‡ä»¶å¯é€‰AESåŠ å¯†å­˜å‚¨

---

## 11. å‚è€ƒèµ„æ–™

**ç°æœ‰å·¥å…·**:
- [Everything - voidtools](https://www.voidtools.com/)
- [R-Studio Data Recovery Software](https://www.r-studio.com/)
- [WizTree - The Fastest Disk Space Analyzer](https://diskanalyzer.com/)
- [TreeSize â€“ Official Free Download](https://www.jam-software.com/treesize)
- [WinDirStat - Windows Directory Statistics](https://windirstat.net/)

**æŠ€æœ¯æ–‡æ¡£**:
- [NTFS MFT Structure](https://docs.microsoft.com/en-us/windows/win32/fileio/master-file-table)
- [USN Change Journal](https://docs.microsoft.com/en-us/windows/win32/fileio/change-journals)
- [Python watchdog](https://github.com/gorakhargosh/watchdog)

**æ•°æ®å¯è§†åŒ–**:
- [ECharts Treemap](https://echarts.apache.org/examples/en/editor.html?c=treemap-disk)
- [squarify - Pure Python Treemap Layout](https://github.com/laserson/squarify)

---

## 12. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç”¨æˆ·åé¦ˆ**: ç¡®è®¤åŠŸèƒ½éœ€æ±‚ä¼˜å…ˆçº§
2. **æŠ€æœ¯é¢„ç ”**: éªŒè¯MFTè¯»å–åœ¨ç›®æ ‡ç¯å¢ƒå¯è¡Œæ€§
3. **åŸå‹å¼€å‘**: å…ˆå®ç°æ ¸å¿ƒç´¢å¼•å¼•æ“å’Œç©ºé—´æ³„æ¼è¯Šæ–­
4. **è¿­ä»£ä¼˜åŒ–**: æ ¹æ®å®é™…ä½¿ç”¨åé¦ˆè°ƒæ•´åŠŸèƒ½

---

**ç‰ˆæœ¬**: v1.0.0
**åˆ›å»ºæ—¥æœŸ**: 2026-02-05
**ä½œè€…**: Claude (HIDRS Plugin System)
